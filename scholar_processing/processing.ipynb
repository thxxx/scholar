{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "prefix = \"scholar_search_results\"\n",
    "pattern = f\"{prefix}*.csv\"\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "df4 = pd.read_csv('scholar_total.csv')\n",
    "dfs = [df4]\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(e, path)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(len(merged))\n",
    "clean = merged.drop_duplicates(subset=[\"author_id\"], keep='first')\n",
    "print(len(clean))\n",
    "\n",
    "clean.to_csv(\"scholar_from_google.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 토픽이 논문 제목에 포함된 것만 남기기\n",
    "deep_learning_keywords = [\n",
    "    'llm',\n",
    "    'agent',\n",
    "    'large model',\n",
    "    'multimodal',\n",
    "    'adversarial network',\n",
    "    'generation',\n",
    "    \n",
    "    # Core DL fields\n",
    "    \"deep learning\",\n",
    "    'machine learning',\n",
    "    \"neural network\",\n",
    "    \"transformer\",\n",
    "    \"attention\",\n",
    "    \"autoregressive\",\n",
    "    \"self-supervised\",\n",
    "    \"contrastive learning\",\n",
    "    \"representation learning\",\n",
    "    \"data-driven\",\n",
    "    \"latent space\",\n",
    "    \"resnet\",\n",
    "    \"convnext\",\n",
    "    \"encoder\",\n",
    "    \"decoder\",\n",
    "    \"autoencoder\",\n",
    "    \"codec\",\n",
    "    \"nerf\",\n",
    "    \"gaussian\",\n",
    "    \"splatting\",\n",
    "    \"neural\",\n",
    "    \"super resolution\",\n",
    "\n",
    "    # Architectures\n",
    "    \"cnn\",\n",
    "    \"lstm\",\n",
    "    \"unet\",\n",
    "    \"vit\",\n",
    "    \"bert\",\n",
    "    \"gpt\",\n",
    "    \"vae\",\n",
    "    \"gan\",\n",
    "    \"vq-vae\",\n",
    "\n",
    "    # Generative models\n",
    "    \"diffusion\",\n",
    "    \"diffusion model\",\n",
    "    \"score-based\",\n",
    "    \"flow matching\",\n",
    "    \"normalizing flow\",\n",
    "    \"generative model\",\n",
    "    \n",
    "    # Multimodal\n",
    "    \"multimodal\",\n",
    "    \"vision-language\",\n",
    "    \"audio-language\",\n",
    "    \"clip model\",\n",
    "    \"large language model\",\n",
    "\n",
    "    # Audio / speech\n",
    "    \"tts\",\n",
    "    \"speech recognition\",\n",
    "    \"audio generation\",\n",
    "    \"music generation\",\n",
    "\n",
    "    \"backpropagation\",\n",
    "    \"gradient descent\",\n",
    "    \"finetuning\",\n",
    "    \"fine-tuning\",\n",
    "    \"pretraining\",\n",
    "    \"pre-training\",\n",
    "    \"zero-shot\",\n",
    "    \"zeroshot\",\n",
    "    \"few-shot\",\n",
    "    \"fewshot\",\n",
    "    \"kl divergence\",\n",
    "    \"cross entropy\",\n",
    "\n",
    "    \"reinforcement learning\",\n",
    "    \"policy gradient\",\n",
    "    \"ppo\",\n",
    "    \"grpo\",\n",
    "    \"rlhf\",\n",
    "    \"human feedback\",\n",
    "    \"reward model\",\n",
    "    \"graph\",\n",
    "    \"feedforward\",\n",
    "    \"feed-forward\",\n",
    "    \"backward\",\n",
    "    \"back-ward\",\n",
    "    \"representation\",\n",
    "    \"training \",\n",
    "    \"hyperparameter\",\n",
    "    \"hyper-parameter\",\n",
    "    \"supervised learning\",\n",
    "\n",
    "    \"retrieval\",\n",
    "    \"quantization\",\n",
    "    \"vlm \",\n",
    "    \"sllm\",\n",
    "\n",
    "    \"computer\",\n",
    "    \"algorithm\",\n",
    "\n",
    "    \"system\",\n",
    "    \"efficient\",\n",
    "    \"efficient\",\n",
    "]\n",
    "\n",
    "surnames = [\n",
    "    \"Kang\", \"Ko\", \"Gwak\", \"Gu\", \"Guk\", \"Kwon\", \"Keum\", \"Ki\",\n",
    "    \"Na\", \"Nam\", \"Namgung\", \"Noh\", \"Non\", \"Dan\", \"Dam\", \"Dang\",\n",
    "    \"Do\", \"Dokgo\", \"Dongbang\", \"Dong\", \"Du\", \"Ra\", \"Ryeong\",\n",
    "    \"Ryu\", \"Ryuk\", \"Ri\",\n",
    "    \"Ma\", \"Man\", \"Maeng\", \"Myeong\", \"Mo\", \"Mok\", \"Muk\", \"Moon\",\n",
    "    \"Min\", \"Park\", \"Ban\", \"Bang\", \"Bae\", \"Baek\", \"Beom\", \"Byun\",\n",
    "    \"Bok\", \"Bong\", \"Boo\", \"Bi\", \"Bin\",\n",
    "    \"Sa\", \"Sam\", \"Sang\", \"Seo\", \"Seomun\", \"Seon\", \"Seonwoo\", \"Sung\",\n",
    "    \"So\", \"Son\", \"Song\", \"Su\", \"Seung\", \"Si\", \"Shin\", \"Sim\",\n",
    "    \"A\", \"Ahn\", \"Ae\", \"Yang\", \"Eo\", \"Eom\", \"Yeo\", \"Yeon\", \"Yeom\",\n",
    "    \"Young\", \"Ye\", \"Oh\", \"Ok\", \"On\", \"Ong\", \"Wang\", \"Yo\", \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Wi\", \"Yu\", \"Yuk\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jeol\", \"Jung\", \"Je\", \"Jegal\",\n",
    "    \"Jo\", \"Jwa\", \"Joo\", \"Juk\", \"Jun\", \"Ji\", \"Jin\",\n",
    "    \"Cha\", \"Chae\", \"Cheo\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Chu\", \"Tak\", \"Tan\", \"Tang\", \"Tae\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\", \"Hyun\", \"Hyeong\", \"Ho\",\n",
    "    \"Hong\", \"Hwa\", \"Hwang\", \"Hwangbo\", \"Hu\", \"Heung\"\n",
    "]\n",
    "surnames = [\n",
    "    \"Kang\", \"Kwon\", \"Ko\"\n",
    "    \"Park\", \"Bang\", \"Bae\", \"Baek\",\n",
    "    \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jung\",\n",
    "    \"Jo\", \"Jin\",\n",
    "    \"Cha\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\",\n",
    "    \"Kim\",\n",
    "    \"Hong\", \"Hwang\",\n",
    "    \"Shin\", \"Song\", \"Seo\",\n",
    "    \"Kwak\", \"Park\", \"Ryu\", \"Soh\", \"Roh\", \"Im\", \"Ahn\", \"Koh\",\n",
    "    \"Nam\", \"Oh\", \"Huh\", \"Son\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 0) Load data\n",
    "# -----------------------------------------------------------\n",
    "from_scholar = pd.read_csv(\"scholar_from_scholar_search_total.csv\")\n",
    "\n",
    "print(\"\\n=== BASIC INFO ===\")\n",
    "print(f\"Total rows in from_scholar: {len(from_scholar):,}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) Unique authors\n",
    "# -----------------------------------------------------------\n",
    "author_unique = from_scholar.drop_duplicates(subset=[\"author_id\"], keep=\"first\")\n",
    "print(f\"Unique authors: {len(author_unique):,}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Deep Learning 관련 논문 필터링\n",
    "#    - deep_learning_keywords: 미리 정의되어 있다고 가정\n",
    "# -----------------------------------------------------------\n",
    "pattern_dl = \"|\".join(deep_learning_keywords)\n",
    "\n",
    "# title에서 DL 키워드 포함된 논문만\n",
    "df_dl = from_scholar[\n",
    "    from_scholar[\"title\"]\n",
    "    .fillna(\"\")\n",
    "    .str.lower()\n",
    "    .str.contains(pattern_dl.lower(), regex=True, na=False)\n",
    "]\n",
    "\n",
    "# 딥러닝 논문을 쓴 author_id 기준 unique\n",
    "df_dl_unique = df_dl.drop_duplicates(subset=[\"author_id\"], keep=\"first\")\n",
    "print(\"\\n=== DEEP LEARNING FILTER ===\")\n",
    "print(f\"Authors with DL-related papers : {len(df_dl_unique):,}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) '논문이 1개뿐인 사람' 구하기 (딥러닝 여부 상관 X)\n",
    "# -----------------------------------------------------------\n",
    "paper_counts = from_scholar.groupby(\"author_id\").size()\n",
    "one_paper_ids = set(paper_counts[paper_counts == 1].index)\n",
    "print(f\"Authors with exactly 1 paper   : {len(one_paper_ids):,}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) '딥러닝 논문 있거나(or) 논문이 1개뿐인 사람' 집합 만들기\n",
    "# -----------------------------------------------------------\n",
    "dl_ids = set(df_dl_unique[\"author_id\"])\n",
    "target_ids = dl_ids.union(one_paper_ids)\n",
    "\n",
    "print(\"\\n=== TARGET AUTHORS (DL OR 1 PAPER) ===\")\n",
    "print(f\"DL authors (>=1 DL paper)      : {len(dl_ids):,}\")\n",
    "print(f\"Union(DL authors, 1-paper)     : {len(target_ids):,}\")\n",
    "\n",
    "# 이 target_ids에 해당하는 author들의 대표 row 하나씩만 유지\n",
    "df_target_authors = (\n",
    "    from_scholar[from_scholar[\"author_id\"].isin(target_ids)]\n",
    "    .drop_duplicates(subset=[\"author_id\"], keep=\"first\")\n",
    "    .copy()\n",
    ")\n",
    "print(f\"Target author rows (unique)    : {len(df_target_authors):,}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) Korean surnames 필터링\n",
    "#    - surnames: 한국 성 리스트 (예: [\"kim\", \"park\", \"lee\", ...])\n",
    "#    - \" kim\", \" park\" 처럼 공백 포함해서 패턴 생성\n",
    "# -----------------------------------------------------------\n",
    "pattern_kr = \"|\".join([\" \" + s.lower() for s in surnames])\n",
    "\n",
    "df_target_authors[\"author_lower\"] = df_target_authors[\"author_names\"].fillna(\"\").str.lower()\n",
    "\n",
    "df_korean_final = df_target_authors[\n",
    "    df_target_authors[\"author_lower\"].str.contains(pattern_kr, regex=True, na=False)\n",
    "].copy()\n",
    "\n",
    "print(\"\\n=== KOREAN AUTHORS (DL OR 1 PAPER) ===\")\n",
    "print(f\"Korean DL/1-paper authors      : {len(df_korean_final):,}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) SUMMARY\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total rows in original CSV     : {len(from_scholar):,}\")\n",
    "print(f\"Total unique authors           : {len(author_unique):,}\")\n",
    "print(f\"DL authors (>=1 DL paper)      : {len(df_dl_unique):,}\")\n",
    "print(f\"Authors with exactly 1 paper   : {len(one_paper_ids):,}\")\n",
    "print(f\"Target authors (DL or 1 paper) : {len(df_target_authors):,}\")\n",
    "print(f\"Korean target authors          : {len(df_korean_final):,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_counts = df_dl.groupby('author_id').size()\n",
    "\n",
    "def bucket_paper_count(n):\n",
    "    if n == 1:\n",
    "        return \"1 paper\"\n",
    "    elif n == 2:\n",
    "        return \"2 papers\"\n",
    "    elif n == 3:\n",
    "        return \"3 papers\"\n",
    "    elif 4 <= n <= 5:\n",
    "        return \"4-5 papers\"\n",
    "    else:\n",
    "        return \"6+ papers\"\n",
    "buckets = paper_counts.map(bucket_paper_count)\n",
    "import pandas as pd\n",
    "\n",
    "bucket_counts = buckets.value_counts()\n",
    "\n",
    "# 보기 좋게 순서 고정\n",
    "bucket_order = [\"1 paper\", \"2 papers\", \"3 papers\", \"4-5 papers\", \"6+ papers\"]\n",
    "bucket_counts = bucket_counts.reindex(bucket_order, fill_value=0)\n",
    "\n",
    "total_authors = len(paper_counts)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"count\": bucket_counts,\n",
    "    \"ratio\": (bucket_counts / total_authors).round(4)  # 비율\n",
    "})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7495e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "datas = []\n",
    "\n",
    "from_google = pd.read_csv('scholar_from_google.csv')\n",
    "\n",
    "for i in tqdm(range(len(from_scholar_deep_learning_korean))):\n",
    "    d = from_scholar_deep_learning_korean.iloc[i]\n",
    "    datas.append({\n",
    "        'name': d['author_names'],\n",
    "        'author_id': d['author_id'],\n",
    "    })\n",
    "\n",
    "for i in tqdm(range(len(from_google))):\n",
    "    d = from_google.iloc[i]\n",
    "    datas.append({\n",
    "        'name': d['name'],\n",
    "        'author_id': d['author_id'],\n",
    "    })\n",
    "print(len(datas))\n",
    "\n",
    "newdf = pd.DataFrame(datas)\n",
    "newdf = newdf.drop_duplicates(subset=['author_id'], keep='first')\n",
    "print(len(newdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbfc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_korean_final.to_csv(\"deep_or_one_korean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa0c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef067345",
   "metadata": {},
   "source": [
    "### make coauthor list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65381a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "current_coauthor_map = json.load(open('coauthors.json'))\n",
    "print(len(current_coauthor_map))\n",
    "extracted_authors = pd.read_csv(\"most_extracted_authors.csv\")\n",
    "\n",
    "for i in range(len(extracted_authors)):\n",
    "    d = extracted_authors.iloc[i]\n",
    "    \n",
    "    # from_author_id가 NaN이면 skip\n",
    "    if pd.isna(d['from_author_id']):\n",
    "        continue\n",
    "    \n",
    "    a = str(d['author_id'])\n",
    "    b = str(d['from_author_id'])\n",
    "    \n",
    "    # 딕셔너리에 키 없으면 먼저 만들어주기\n",
    "    if a not in current_coauthor_map:\n",
    "        current_coauthor_map[a] = []\n",
    "    if b not in current_coauthor_map:\n",
    "        current_coauthor_map[b] = []\n",
    "\n",
    "    # 양방향 추가 (중복 방지)\n",
    "    if b not in current_coauthor_map[a]:\n",
    "        current_coauthor_map[a].append(b)\n",
    "    if a not in current_coauthor_map[b]:\n",
    "        current_coauthor_map[b].append(a)\n",
    "\n",
    "# Save\n",
    "print(len(current_coauthor_map))\n",
    "json.dump(current_coauthor_map, open('coauthors.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0349bcf",
   "metadata": {},
   "source": [
    "### After get detail profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "prefix = \"./middle_files/detail_profiles\"\n",
    "pattern = f\"{prefix}*.csv\"\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "dfs = []\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(e, path)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(len(merged))\n",
    "\n",
    "merged.to_csv(\"1212_scholar_detail_profiles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"1212_scholar_detail_profiles.csv\")\n",
    "df2 = pd.read_csv(\"most_recent_total_detail_profiles.csv\")\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(len(df))\n",
    "\n",
    "df = df.drop_duplicates(subset=['author_id'], keep='first')\n",
    "print(len(df))\n",
    "\n",
    "df.to_csv(\"most_recent_total_detail_profiles.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "prefix = \"./middle_files/extracted_authors\"\n",
    "pattern = f\"{prefix}*.csv\"\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "dfs = []\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(e, path)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(len(merged))\n",
    "\n",
    "merged.to_csv(\"1212_extracted_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e4858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b850a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interests = set([])\n",
    "\n",
    "for intersts_list in df_filtered_not['interests_list']:\n",
    "    for interest in intersts_list:\n",
    "        interests.add(interest)\n",
    "\n",
    "interests\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for interests in df['interests_list']:\n",
    "    text = \" \".join(interests).lower()\n",
    "    for kw in dl_keywords:\n",
    "        if kw in text:\n",
    "            counter[kw] += 1\n",
    "\n",
    "keyword_counts = dict(sorted(counter.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c19fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_interests = [\n",
    "    \"\"\n",
    "    # Core AI / ML\n",
    "    \"artificial intelligence\", \"machine learning\", \"deep learning\", \"neural networks\",\n",
    "    \"representation learning\", \"computational intelligence\", \"statistical learning\",\n",
    "    \"pattern recognition\", \"data mining\", \"optimization\", \"reinforcement learning\",\n",
    "    \"probabilistic modeling\", \"generative models\", \"foundation models\",\n",
    "\n",
    "    # Deep Learning Subfields\n",
    "    \"computer vision\", \"image processing\", \"image recognition\", \"object detection\",\n",
    "    \"semantic segmentation\", \"instance segmentation\", \"image classification\",\n",
    "    \"video understanding\", \"video analysis\", \"3d vision\", \"scene understanding\",\n",
    "    \"medical image analysis\", \"computational photography\", \"robot perception\",\n",
    "\n",
    "    # NLP / Speech\n",
    "    \"natural language processing\", \"nlp\", \"text mining\", \"question answering\",\n",
    "    \"sentiment analysis\", \"language modeling\", \"speech recognition\",\n",
    "    \"speech processing\", \"speech synthesis\", \"tts\", \"asr\", \"dialog systems\",\n",
    "    \"multimodal learning\",\n",
    "\n",
    "    # Audio / Signal\n",
    "    \"audio processing\", \"audio signal processing\", \"music information retrieval\",\n",
    "    \"sound event detection\",\n",
    "\n",
    "    # Model Architecture / Optimization\n",
    "    \"transformers\", \"attention\", \"graph neural networks\", \"gnn\",\n",
    "    \"convolutional neural networks\", \"cnn\", \"rnn\", \"lstm\", \"vae\", \"gan\",\n",
    "    \"diffusion models\", \"flow matching\", \"autoregressive models\",\n",
    "\n",
    "    # Robotics / Control\n",
    "    \"robotics\", \"robot control\", \"robot learning\", \"autonomous systems\",\n",
    "\n",
    "    # Recommendation / Info Retrieval\n",
    "    \"information retrieval\", \"recommender systems\", \"recommendation systems\",\n",
    "    \"knowledge graphs\",\n",
    "\n",
    "    # Other ML Areas\n",
    "    \"meta-learning\", \"federated learning\", \"domain adaptation\",\n",
    "    \"transfer learning\", \"self-supervised learning\", \"semi-supervised learning\",\n",
    "    \"unsupervised learning\", \"online learning\", \"active learning\",\n",
    "\n",
    "    # Applied ML\n",
    "    \"medical ai\", \"bioinformatics\", \"healthcare ai\", \"financial ai\",\n",
    "    \"computational biology\", \"autonomous driving\", \"human-computer interaction\",\n",
    "    \"llm\", \"llms\", \"large language models\", \"large language model\", 'diffusion', 'generative model',\n",
    "\n",
    "    # Robustness / Safety / Explainability\n",
    "    \"robustness\", \"interpretability\", \"explainable ai\", \"adversarial machine learning\",\n",
    "    \"fairness\", \"ai safety\",\n",
    "\n",
    "    # Training / Optimization Topics\n",
    "    \"loss functions\", \"sampling\", \"regularization\", \"bayesian optimization\",\n",
    "    \"stochastic optimization\", \"large scale learning\",\n",
    "\n",
    "    # Agents\n",
    "    \"agent\", \"multi-agent systems\", \"autonomous agents\", \"AI \", \" AI\", \"reinforcement\", \"supervised\", \"rlhf\", \"ppo\", 'finetuning', '3d', 'nerf', 'video', \"image\",\n",
    "\n",
    "    'ML',\n",
    "    'Multimodal Understanding',\n",
    "    'Adversarial attacks',\n",
    "    'Anomaly detection',\n",
    "    'Tensor Mining',\n",
    "    'AI-Embedded Software-on-Chip Lab',\n",
    "    'Inference Serving',\n",
    "    'neural network',\n",
    "    'AI/ML/DL',\n",
    "    'Automated Reasoning',\n",
    "    'speech signal processing',\n",
    "    'XAI',\n",
    "    'Neural computation',\n",
    "    'learning model',\n",
    "    'Sim2Real',\n",
    "    'Multimodal Discourse Analysis',\n",
    "    'Action Recognition',\n",
    "    'Neural Engineeing',        # == Neural Engineering\n",
    "    'User Interface and Interaction',  # borderline, but UI+AI 연구 가능\n",
    "    'computational methods',    # broad, but usually ML-heavy\n",
    "    'data-driven analysis',\n",
    "    'Neural coding',\n",
    "    'Network Neuroscience',     # computational modeling domain\n",
    "    'Neural interfaces',        # sometimes ML-driven\n",
    "    'systems and computational neuroscience',\n",
    "    'Causal inference',         # ML/AI research area\n",
    "    'Computer security',        # ML subfield when tied to adversarial ML\n",
    "    'Time Series Forcasting',   # (typo: Forecasting) common ML area\n",
    "    'Gradient boosting',        # ML technique\n",
    "    'Deep learining',           # typo → deep learning\n",
    "    'Vision Language Models',\n",
    "    'Multimodal Discourse Analysis',\n",
    "    'Multiphysics modeling',    # borderline\n",
    "    'Inference Serving',\n",
    "    'artificial neural network',\n",
    "    'artificial general intelligence',\n",
    "    'computation',\n",
    "    'computer'\n",
    "]\n",
    "dl_keywords = [kw.lower() for kw in dl_interests]\n",
    "\n",
    "def has_dl_interest_partial(interests):\n",
    "    text = \" \".join(interests).lower()  # 하나의 문자열처럼 붙여서 체크\n",
    "    return any(kw in text for kw in dl_keywords)\n",
    "\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(\"most_recent_total_detail_profiles.csv\")\n",
    "print(len(df))\n",
    "\n",
    "df['interests_list'] = df['interests'].apply(lambda x: ast.literal_eval(x))\n",
    "df_deeplearning_related = df[df['interests_list'].apply(has_dl_interest_partial)]\n",
    "print(len(df_deeplearning_related))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = [\n",
    "    \"Kang\", \"Kwon\", \"Ko\"\n",
    "    \"Park\", \"Bang\", \"Bae\", \"Baek\",\n",
    "    \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jung\",\n",
    "    \"Jo\", \"Jin\",\n",
    "    \"Cha\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\",\n",
    "    \"Kim\",\n",
    "    \"Hong\", \"Hwang\",\n",
    "    \"Shin\", \"Song\", \"Seo\",\n",
    "    \"Kwak\", \"Park\", \"Ryu\", \"Soh\", \"Roh\", \"Im\", \"Ahn\", \"Koh\",\n",
    "    \"Nam\", \"Oh\", \"Huh\", \"Son\"\n",
    "]\n",
    "pattern = \"|\".join([' ' + s.lower() for s in surnames])\n",
    "\n",
    "coauthors = pd.read_csv(\"1211_extracted_authors.csv\")\n",
    "\n",
    "# remains = coauthors.drop_duplicates(subset=['author_id'], keep='last')\n",
    "korean = coauthors[coauthors[\"author_names\"].str.lower().str.contains(pattern, regex=True)]\n",
    "print(len(korean))\n",
    "\n",
    "deeps = set(df['author_id'].tolist())\n",
    "from_deep_coauthor_korean_ids = set([])\n",
    "\n",
    "for i in range(len(korean)):\n",
    "    d = korean.iloc[i]\n",
    "    if d['from_author_id'] in deeps and d['author_id'] not in deeps:\n",
    "        from_deep_coauthor_korean_ids.add(d['author_id'])\n",
    "\n",
    "print(len(from_deep_coauthor_korean_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad728da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_korean_final)):\n",
    "    d = df_korean_final.iloc[i]\n",
    "    if d['author_id'] not in deeps:\n",
    "        from_deep_coauthor_korean_ids.add(d['author_id'])\n",
    "print(len(from_deep_coauthor_korean_ids))\n",
    "\n",
    "datas = []\n",
    "for sf in from_deep_coauthor_korean_ids:\n",
    "    datas.append({\n",
    "        'author_id': sf,\n",
    "    })\n",
    "\n",
    "newdf = pd.DataFrame(datas)\n",
    "newdf.to_csv(\"from_coauthor_1211.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf363d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c431fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30b491a",
   "metadata": {},
   "source": [
    "### Scraping Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ec3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"most_recent_total_detail_profiles.csv\")\n",
    "print(len(df))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5390dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "citations_ids = set([])\n",
    "papers_citations = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(len(df_deeplearning_related))):\n",
    "    d = df_deeplearning_related.iloc[i]\n",
    "    if pd.isna(d['articles']):\n",
    "        continue\n",
    "    \n",
    "    for c in ast.literal_eval(d['articles']):\n",
    "        citations_ids.add(f\"{c['title'].lower()}_{c['year']}\")\n",
    "        # papers_citations[c['citation_id'].split(\":\")[1]].append(c['title'])\n",
    "\n",
    "print(len(citations_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google_scholar/author\"\n",
    "author_id = \"E0YZuSgAAAAJ\"\n",
    "citation_id = \"E0YZuSgAAAAJ:4OULZ7Gr8RgC\"\n",
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    \"author_id\": author_id,\n",
    "    \"citation_id\": citation_id,\n",
    "    \"view_op\": \"view_citation\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "data = {\n",
    "    \"author_id\" : author_id,\n",
    "    \"citation_id\" : citation_id,\n",
    "    \"title\": data['title'],\n",
    "    \"link\": data['link'],\n",
    "    \"description\": data['description'],\n",
    "    \"publication_date\": data['publication_date']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e83a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install apify-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d718b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_client import ApifyClient\n",
    "\n",
    "# Initialize the ApifyClient with your API token\n",
    "client = ApifyClient(\"\")\n",
    "\n",
    "# Prepare the Actor input\n",
    "run_input = { \"profileUrls\": [\n",
    "        \"\",\n",
    "        \"\"\n",
    "    ] }\n",
    "\n",
    "# Run the Actor and wait for it to finish\n",
    "run = client.actor(\"2SyF0bVxmgGr8IVCZ\").call(run_input=run_input)\n",
    "\n",
    "# Fetch and print Actor results from the run's dataset (if there are any)\n",
    "items = []\n",
    "\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    print(item)\n",
    "    items.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a29e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c611d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40662\n",
      "40591\n",
      "40662\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"most_recent_total_detail_profiles.csv\")\n",
    "print(len(df))\n",
    "hdf = pd.read_csv(\"only_home_links.csv\")\n",
    "hdf = hdf.drop_duplicates(subset=['author_id'])\n",
    "print(len(hdf))\n",
    "\n",
    "# author_id 기준으로 homepage 컬럼 붙이기\n",
    "merged = df.merge(\n",
    "    hdf[[\"author_id\", \"home_link\"]],\n",
    "    on=\"author_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(len(merged))\n",
    "\n",
    "# merged.to_csv(\n",
    "#     \"most_recent_total_detail_profiles_with_homepage.csv\",\n",
    "#     index=False,\n",
    "#     encoding=\"utf-8-sig\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f758d115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23674"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged[pd.isna(merged['home_link'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c65390a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged.to_csv(\n",
    "    \"most_recent_total_detail_profiles_with_homepage.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe6530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
