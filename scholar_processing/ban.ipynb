{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f324d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (25.3)\n",
      "Requirement already satisfied: openai in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: requests in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from beautifulsoup4) (4.15.0)\n",
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "!python3 -m pip install openai\n",
    "!python3 -m pip install requests\n",
    "!python3 -m pip install beautifulsoup4\n",
    "!clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f88e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40662\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import re\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://api.x.ai/v1/\",\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"most_recent_total_detail_profiles_with_homepage.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59c67ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_text(soup):\n",
    "    NOISE_ID_RE = re.compile(\n",
    "        r\"^(actionbar|jp-|jetpack|wpgroho|grofiles|wp-emoji|comment-|bilmur|follow-bubble)\",\n",
    "        re.I,\n",
    "    )\n",
    "\n",
    "    GENERIC_TAGS = [\n",
    "        \"div\", \"section\", \"article\", \"main\",\n",
    "        \"header\", \"footer\", \"nav\", \"span\", \"p\"\n",
    "    ]\n",
    "\n",
    "    # Common \"noisy\" attributes added by frameworks/trackers/renderers\n",
    "    DROP_ATTR_PREFIXES = (\"data-\", \"aria-\")\n",
    "    DROP_ATTR_NAMES = {\n",
    "        \"role\", \"rel\", \"target\", \"tabindex\",\n",
    "        # \"itemprop\", \"itemscope\", \"itemtype\",\n",
    "        \"contenteditable\", \"draggable\", \"spellcheck\",\n",
    "        \"loading\", \"decoding\", \"fetchpriority\",\n",
    "        \"srcset\", \"sizes\", \"integrity\", \"crossorigin\",\n",
    "        \"referrerpolicy\", \"nonce\",\n",
    "        \"width\", \"height\",\n",
    "    }\n",
    "    ON_EVENT_ATTR_RE = re.compile(r\"^on[a-z]+$\", re.I)\n",
    "\n",
    "    def strip_noisy_nodes_and_attrs(root):\n",
    "        # Remove comments\n",
    "        for c in root.find_all(string=lambda s: isinstance(s, Comment)):\n",
    "            c.extract()\n",
    "\n",
    "        # Remove whole noisy tags\n",
    "        for t in root.find_all([\"script\", \"style\", \"noscript\", \"template\", \"svg\", \"canvas\"]):\n",
    "            t.decompose()\n",
    "\n",
    "        # Remove noisy attributes\n",
    "        for t in root.find_all(True):\n",
    "            attrs = list(t.attrs.keys())\n",
    "            for k in attrs:\n",
    "                kl = k.lower()\n",
    "\n",
    "                # Drop inline event handlers like onclick/onload/...\n",
    "                if ON_EVENT_ATTR_RE.match(kl):\n",
    "                    t.attrs.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "                # Drop data-*, aria-*\n",
    "                if any(kl.startswith(p) for p in DROP_ATTR_PREFIXES):\n",
    "                    t.attrs.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "                # Drop known noisy attributes\n",
    "                if kl in DROP_ATTR_NAMES:\n",
    "                    t.attrs.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "    def remove_class_style_regex(html: str) -> str:\n",
    "        # Remove class/style in both quote styles\n",
    "        html = re.sub(r\"\\sclass=\\\"[^\\\"]*\\\"\", \"\", html)\n",
    "        html = re.sub(r\"\\sclass='[^']*'\", \"\", html)\n",
    "        html = re.sub(r\"\\sstyle=\\\"[^\\\"]*\\\"\", \"\", html)\n",
    "        html = re.sub(r\"\\sstyle='[^']*'\", \"\", html)\n",
    "        return html\n",
    "\n",
    "    def shrink_html_tokens(html) -> str:\n",
    "        # First pass: remove noisy nodes/attrs (framework/tracker bloat)\n",
    "        strip_noisy_nodes_and_attrs(html)\n",
    "\n",
    "        # Second pass: remove Jetpack/WordPress-ish noisy blocks by id/class pattern\n",
    "        for t in list(html.find_all(True)):\n",
    "            if t is None:\n",
    "                continue\n",
    "\n",
    "            tid = t.get(\"id\") or \"\"\n",
    "            if tid and NOISE_ID_RE.match(tid):\n",
    "                t.decompose()\n",
    "                continue\n",
    "\n",
    "            classes = \" \".join(t.get(\"class\", [])) if t.get(\"class\") else \"\"\n",
    "            if classes and NOISE_ID_RE.search(classes):\n",
    "                t.decompose()\n",
    "                continue\n",
    "\n",
    "        s = str(html)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        s = s.replace(\"\\t\", \"\")\n",
    "        s = re.sub(r\"\\n+\", \"\\n\", s)\n",
    "        s = re.sub(r\"[ ]{2,}\", \" \", s)\n",
    "\n",
    "        # Collapse generic tags\n",
    "        for tag in GENERIC_TAGS:\n",
    "            s = re.sub(fr\"<{tag}(\\s[^>]*)?>\", \"<>\", s)\n",
    "            s = re.sub(fr\"</{tag}>\", \"</>\", s)\n",
    "\n",
    "        # Collapse empty generic pairs\n",
    "        s = re.sub(r\"<>\\s*</>\", \"<>\", s)\n",
    "\n",
    "        # Remove inter-tag whitespace\n",
    "        s = re.sub(r\">\\s+<\", \"><\", s)\n",
    "\n",
    "        return s.strip()\n",
    "\n",
    "    # Usage\n",
    "    body = soup.find(\"body\")\n",
    "    sp = shrink_html_tokens(body)\n",
    "    sp = remove_class_style_regex(sp)\n",
    "    \n",
    "    return sp\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an information extraction assistant.\n",
    "\n",
    "Below is a raw HTML document of a person's profile website.\n",
    "Your task is to extract the following attributes **only if they are explicitly present in the HTML**.\n",
    "Do NOT infer, guess, or hallucinate any information.\n",
    "\n",
    "If an attribute cannot be found with reasonable certainty, return `None` for that field.\n",
    "\n",
    "### Attributes to extract\n",
    "- email: A contact email address of the owner found in the HTML (e.g., mailto links or visible text). only the owner's email address, not other people.\n",
    "- related_links: a list of links that appear to be related to the exact person. (e.g. linkedin, github, cv, blog, sns, etc.). Do not include every single paper, citation, project, labs or company links.\n",
    "- bio: A short biography or description text describing the person, company, or project. Write it in the first person. At leat 3 sentences, at most 6 sentences.\n",
    "e.g. I am the researcher who is interested in ...\n",
    "- page_type: If current html is not a profile page, return the type of the page. e.g. \"company\", \"blog\", \"other\", \"labs\", etc.\n",
    "If is not a profile page, return None for all the other attributes.\n",
    "- company_experiences: A list of company experiences of the owner. include the company name, title(Role), start date, end date.\n",
    "ex) \n",
    "\"company_experiences\": [\n",
    "    {\n",
    "      \"company_name\": \"Company A\",\n",
    "      \"title\": \"Reseach Scientist, TTS end LLM optimization\",\n",
    "      \"start_date\": \"2020-07\",\n",
    "      \"end_date\": \"2021-04\"\n",
    "    },{\n",
    "      \"company_name\": \"Los University\",\n",
    "      \"title\": \"Assistant professor\",\n",
    "      \"start_date\": \"2019-10\",\n",
    "      \"end_date\": \"2018-02\"\n",
    "    }, etc\n",
    "]\n",
    "- education: A list of education experiences of the owner. include the school name, degree, start date, end date. only include BS, MS, PhD. no \n",
    "ex)\n",
    "\"education\": [\n",
    "    {\n",
    "      \"school_name\": \"School A\",\n",
    "      \"degree\": \"Ph.D. in Computer Science\",\n",
    "      \"start_date\": \"2018-11\",\n",
    "      \"end_date\": \"2022-01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "Write date in format \"YYYY-MM\"\n",
    "\n",
    "### Output format\n",
    "Return a **valid JSON object** exactly in the following format:\n",
    "\n",
    "{\n",
    "  \"email\": string | None,\n",
    "  \"related_links\": list[string] | None,\n",
    "  \"bio\": string | None,\n",
    "  \"page_type\": string,\n",
    "  \"company_experiences\": list[dict] | None,\n",
    "  \"education\": list[dict] | None,\n",
    "}\n",
    "\n",
    "### Rules\n",
    "- Use `None` (not null, not empty string) if the value is missing\n",
    "- For `related_links`, return `None` if no relevant links are found\n",
    "- Do not include duplicate links\n",
    "- Do not include navigation-only or irrelevant links (e.g., privacy policy, terms)\n",
    "- Preserve the original text as-is (do not paraphrase)\n",
    "- Do not add any explanations or extra text outside the JSON\n",
    "- Only include in related_links of a list of links that appear to be related to the exact person. (e.g. linkedin, github, cv, blog, sns, etc.). Do not include every single paper, citation, project, labs or company links.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "def calc_cost_with_cache(\n",
    "    usage,\n",
    "    input_price_per_1k,\n",
    "    output_price_per_1k,\n",
    "    input_price_per_cached_1k,\n",
    "):\n",
    "    # prompt\n",
    "    total_prompt = usage.prompt_tokens\n",
    "    cached = usage.prompt_tokens_details.cached_tokens or 0\n",
    "    billable_prompt = max(total_prompt - cached, 0)\n",
    "\n",
    "    # completion\n",
    "    completion = usage.completion_tokens\n",
    "\n",
    "    input_cost = billable_prompt / 1000 * input_price_per_1k + cached / 1000 * input_price_per_cached_1k\n",
    "    output_cost = completion / 1000 * output_price_per_1k\n",
    "\n",
    "    return {\n",
    "        \"prompt_tokens_total\": total_prompt,\n",
    "        \"prompt_tokens_cached\": cached,\n",
    "        \"prompt_tokens_billable\": billable_prompt,\n",
    "        \"completion_tokens\": completion,\n",
    "        \"input_cost_usd\": input_cost,\n",
    "        \"output_cost_usd\": output_cost,\n",
    "        \"total_cost_usd\": input_cost + output_cost,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d3a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "http://junsukim.blogspot.kr/\n",
      "author_id                                                         iPSHyTYAAAAJ\n",
      "name                                                                 Junsu Kim\n",
      "affiliations                                      Korea Polytechnic University\n",
      "email                                    Verified email at ieee.org - Homepage\n",
      "interests                                        ['Radio resource management']\n",
      "image_thumbnail              https://scholar.google.com/citations/images/av...\n",
      "articles                     [{'title': 'Mimicking full-duplex relaying usi...\n",
      "total_citation_count                                                     751.0\n",
      "since_2020_citation_count                                                147.0\n",
      "h_index                                                                   17.0\n",
      "home_link                                         http://junsukim.blogspot.kr/\n",
      "Name: 12, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:09<00:52,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hyeonjeong1.notion.site/Hyeonjeong-Shin-36a8827d5e804c388ce0be8afb9fd182\n",
      "author_id                                                         GwF6FmQAAAAJ\n",
      "name                                                           Hyeonjeong Shin\n",
      "affiliations                                               M.S. Student, KAIST\n",
      "email                                 Verified email at kaist.ac.kr - Homepage\n",
      "interests                                                      ['Data Mining']\n",
      "image_thumbnail              https://scholar.google.comhttps://scholar.goog...\n",
      "articles                     [{'title': 'Weather4cast at neurips 2022: Supe...\n",
      "total_citation_count                                                      31.0\n",
      "since_2020_citation_count                                                 31.0\n",
      "h_index                                                                    3.0\n",
      "home_link                    https://hyeonjeong1.notion.site/Hyeonjeong-Shi...\n",
      "Name: 13, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:15<01:05,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ntunedl.weebly.com/\n",
      "author_id                                                         mjwO4sUAAAAJ\n",
      "name                                                                 Munho Kim\n",
      "affiliations                 Associate Professor (Tenured) of EEE at Nanyan...\n",
      "email                                  Verified email at ntu.edu.sg - Homepage\n",
      "interests                    ['Wide bandgap semiconductor', 'Electronics/Op...\n",
      "image_thumbnail              https://scholar.google.comhttps://scholar.goog...\n",
      "articles                     [{'title': 'High-performance green flexible el...\n",
      "total_citation_count                                                    3687.0\n",
      "since_2020_citation_count                                               2928.0\n",
      "h_index                                                                   28.0\n",
      "home_link                                          https://ntunedl.weebly.com/\n",
      "Name: 14, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:31<01:56,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "https://www.linkedin.com/in/park-min-bae-b97543262/\n",
      "author_id                                                         72iRjT8AAAAJ\n",
      "name                                                               Minbae Park\n",
      "affiliations                                                Hanyang university\n",
      "email                               Verified email at hanyang.ac.kr - Homepage\n",
      "interests                             ['RAG', 'LLM', 'GNN', 'Link prediction']\n",
      "image_thumbnail              https://scholar.google.com/citations/images/av...\n",
      "articles                     [{'title': 'ProgRAG: Hallucination-Resistant P...\n",
      "total_citation_count                                                       NaN\n",
      "since_2020_citation_count                                                  NaN\n",
      "h_index                                                                    NaN\n",
      "home_link                    https://www.linkedin.com/in/park-min-bae-b9754...\n",
      "Name: 16, dtype: object\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "https://jkume0107.wordpress.com/\n",
      "author_id                                                         woq_aAEAAAAJ\n",
      "name                                                               Jun'ya Kume\n",
      "affiliations                                              University of Padova\n",
      "email                                    Verified email at unipd.it - Homepage\n",
      "interests                          ['Cosmology', 'Gravitational Wave Physics']\n",
      "image_thumbnail              https://scholar.google.com/citations/images/av...\n",
      "articles                     [{'title': 'GWTC-3: Compact binary coalescence...\n",
      "total_citation_count                                                   12600.0\n",
      "since_2020_citation_count                                              12067.0\n",
      "h_index                                                                   36.0\n",
      "home_link                                     https://jkume0107.wordpress.com/\n",
      "Name: 21, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "https://www.linkedin.com/in/haewookjang/\n",
      "author_id                                                         XHSGWDMAAAAJ\n",
      "name                                                              Jang Haewook\n",
      "affiliations                                                    Meteor Biotech\n",
      "email                                   Verified email at snu.ac.kr - Homepage\n",
      "interests                            ['Bioengineering', 'AI', 'Spatial-omics']\n",
      "image_thumbnail              https://scholar.google.comhttps://scholar.goog...\n",
      "articles                     [{'title': 'Blood culture-free ultra-rapid ant...\n",
      "total_citation_count                                                      93.0\n",
      "since_2020_citation_count                                                 92.0\n",
      "h_index                                                                    4.0\n",
      "home_link                             https://www.linkedin.com/in/haewookjang/\n",
      "Name: 27, dtype: object\n",
      "https://www.linkedin.com/in/jin-s-heo-5043845b\n",
      "author_id                                                         4-NP0K0AAAAJ\n",
      "name                                                 Jin Suck Heo (Jin S. Heo)\n",
      "affiliations                           Samsung Advanced Institue of Technology\n",
      "email                                 Verified email at samsung.com - Homepage\n",
      "interests                    ['Power and Energy Systems', 'Intelligent Cont...\n",
      "image_thumbnail              https://scholar.google.comhttps://scholar.goog...\n",
      "articles                     [{'title': 'Fuel cell system and power managin...\n",
      "total_citation_count                                                    1181.0\n",
      "since_2020_citation_count                                                426.0\n",
      "h_index                                                                   15.0\n",
      "home_link                       https://www.linkedin.com/in/jin-s-heo-5043845b\n",
      "Name: 28, dtype: object\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "        \n",
    "PRICES = {\n",
    "    \"input\": 0.0002,\n",
    "    \"output\": 0.0005,\n",
    "    \"cached\": 0.00005,\n",
    "}\n",
    "\n",
    "total_dollars = 0\n",
    "for i in tqdm(range(10, 30)):\n",
    "    data = df.iloc[i]\n",
    "    hl = data['home_link']\n",
    "\n",
    "    if not pd.isna(hl):\n",
    "        url = hl\n",
    "        print(data)\n",
    "\n",
    "        try:\n",
    "            if \"linkedin.com\" in url:\n",
    "                output = {\n",
    "                    \"email\": '',\n",
    "                    \"related_links\": [url],\n",
    "                    \"bio\": \"\",\n",
    "                    \"page_type\": \"\",\n",
    "                    \"company_experiences\": [],\n",
    "                    \"education\": []\n",
    "                }\n",
    "                print(\"Put it in linkedin.\")\n",
    "                continue\n",
    "            resp = requests.get(\n",
    "                url,\n",
    "                headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "                timeout=30,\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            sp = return_text(soup)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"\\n\\nFailed to get page info\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        prompt = prompt + f\"\"\"\n",
    "### owner's name\n",
    "{data['name']}\n",
    "\n",
    "### HTML Document\n",
    "{sp}\n",
    "\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"grok-4-1-fast-non-reasoning\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a html extractor\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        cost = calc_cost_with_cache(response.usage, PRICES[\"input\"], PRICES[\"output\"], PRICES[\"cached\"])\n",
    "        dollars = cost['total_cost_usd']\n",
    "        total_dollars += dollars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaf5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd72eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35abfd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21993\n",
      "19498\n",
      "41491\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test_with_one.csv')\n",
    "print(len(df))\n",
    "df2 = pd.read_csv('test_with_two.csv')\n",
    "print(len(df2))\n",
    "\n",
    "merged_df = pd.concat([df, df2])\n",
    "print(len(merged_df))\n",
    "\n",
    "merged_df.to_csv('only_home_links.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
