{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "keywords = [\n",
    "    \"model\",\n",
    "    \"generative\",\n",
    "    \"llm\",\n",
    "    \"multimodal\",\n",
    "    \"inference\",\n",
    "    \"benchmark\",\n",
    "    \"image\",\n",
    "    \"audio\",\n",
    "    \"tts\",\n",
    "    \"diffusion\",\n",
    "    \"flow matching\",\n",
    "    \"adversarial\",\n",
    "    \"network\",\n",
    "    \"representation\",\n",
    "    \"training\",\n",
    "    \"loss\",\n",
    "    \"sampling\",\n",
    "    \"agent\",\n",
    "    \"foundation\",\n",
    "    \"recommendation\",\n",
    "    \"robust\",\n",
    "    \"cnn\",\n",
    "    \"learning\",\n",
    "    \"rl\",\n",
    "    \"detection\",\n",
    "    \"retrieval\",\n",
    "    \"denoising\",\n",
    "    \"language\",\n",
    "    \"video\",\n",
    "    \"speech\",\n",
    "    \"reasoning\",\n",
    "    \"policy\",\n",
    "    \"attention\",\n",
    "    \"supervised\",\n",
    "    \"autoregressive\",\n",
    "    \"speech recognition\",\n",
    "    \"synthesis\",\n",
    "    \"probability\",\n",
    "]\n",
    "\n",
    "surnames = [\n",
    "  { \"surname\": \"Kim\",   \"ratio_percent\": 13.5 },\n",
    "  { \"surname\": \"Lee\",   \"ratio_percent\": 8.5 },\n",
    "  { \"surname\": \"Park\",  \"ratio_percent\": 5.5 },\n",
    "  { \"surname\": \"Jung\",  \"ratio_percent\": 2.5 },\n",
    "  { \"surname\": \"Jeong\",  \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Choi\",  \"ratio_percent\": 3.5 },\n",
    "  { \"surname\": \"Cho\",   \"ratio_percent\": 2.5 },\n",
    "  { \"surname\": \"Kang\",  \"ratio_percent\": 2.5 },\n",
    "  { \"surname\": \"Yoon\",  \"ratio_percent\": 2.5 },\n",
    "  { \"surname\": \"Lim\",   \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Jang\",  \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Han\",   \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Oh\",    \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Seo\",   \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Shin\",  \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Kwon\",  \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Hwang\", \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Ahn\",   \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Song\",  \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Ryu\",   \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Hong\",  \"ratio_percent\": 1.5 },\n",
    "  { \"surname\": \"Bae\",  \"ratio_percent\": 1.0 },\n",
    "  { \"surname\": \"Woo\",  \"ratio_percent\": 1.0 },\n",
    "  { \"surname\": \"Yun\",  \"ratio_percent\": 1.0 },\n",
    "  { \"surname\": \"Son\",  \"ratio_percent\": 1.0 },\n",
    "]\n",
    "\n",
    "def sample_hap():\n",
    "    names = [item[\"surname\"] for item in surnames]\n",
    "    weights = [item[\"ratio_percent\"] for item in surnames]\n",
    "    return [random.choices(keywords, k=1)[0].lower(), random.choices(names, weights=weights, k=1)[0].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46e60a27-6591-4714-8cdc-661c75cf44b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Options\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeoutException, WebDriverException, NoSuchElementException, JavascriptException\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException, JavascriptException\n",
    "from selenium_stealth import stealth\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"ko,en;q=0.9\",\n",
    "}\n",
    "\n",
    "\n",
    "# ---------- Selenium ----------\n",
    "def build_driver(headless=True, user_agent=None):\n",
    "    opts = Options()\n",
    "    \n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--no-sandbox')\n",
    "    opts.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    opts.add_experimental_option('excludeSwitches', ['enable-automation', 'enable-logging'])\n",
    "    opts.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    opts.page_load_strategy = \"eager\"\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--window-size=1200,2400\")\n",
    "    opts.add_argument(\"--lang=ko-KR\")\n",
    "\n",
    "    # 리소스 최소화(이미지/CSS 차단)\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,\n",
    "        \"profile.managed_default_content_settings.cookies\": 1,\n",
    "        \"profile.managed_default_content_settings.javascript\": 1,\n",
    "    }\n",
    "    opts.add_experimental_option(\"prefs\", prefs)\n",
    "    if user_agent:\n",
    "        opts.add_argument(f\"--user-agent={user_agent}\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    try:\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"Object.defineProperty(navigator,'webdriver',{get:()=>undefined});\"\n",
    "        })\n",
    "    except Exception:\n",
    "        pass\n",
    "    driver.set_page_load_timeout(2)\n",
    "    \n",
    "    stealth(driver,\n",
    "        languages=['en-US', 'en'],\n",
    "        vendor='Google Inc.',\n",
    "        platform='Win32',\n",
    "        webgl_vendor='Intel Inc.',\n",
    "        renderer='Intel Iris OpenGL Engine',\n",
    "        fix_hairline=True\n",
    "    )\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec70c2f-707d-4700-8368-5629ac1588e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drv = build_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d9654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfdf44-941c-4b72-874d-d23e90f34787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a8a98-edc8-4bc8-b351-98b1f5491950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "author_list = defaultdict(list)\n",
    "paper_list = defaultdict(list)\n",
    "searched_combs = []\n",
    "\n",
    "pds = pd.read_csv(\"output.csv\").to_dict(orient=\"records\")\n",
    "datas = pds\n",
    "\n",
    "def get_titles_and_author_ids(bsoup):\n",
    "    papers = bsoup.find_all(\"div\", class_=re.compile(\"gs_scl\"))\n",
    "    for paper in papers:\n",
    "        try:\n",
    "            title = paper.find(\"h3\", class_=re.compile(\"gs_rt\")).text\n",
    "            author_ids = [re.sub('&hl=en&oi=sra', '', a['href']) for a in paper.find(\"div\", class_=re.compile(\"gs_a\")).find_all(\"a\")]\n",
    "            author_names = [a.text for a in paper.find(\"div\", class_=re.compile(\"gs_a\")).find_all(\"a\")]\n",
    "            matches = re.findall(r\"20\\d{2}\", paper.find(\"div\", class_=re.compile(\"gs_a\")).text)\n",
    "            paper_year = matches[-1]\n",
    "    \n",
    "            ctns = paper.find(\"div\", class_=re.compile(\"gs_flb\")).find_all(\"a\")\n",
    "            if len(ctns)>2:\n",
    "                citation_nums = ctns[2].text\n",
    "                if \"Related\" not in citation_nums:\n",
    "                    citation_nums = citation_nums[len(\"Cited by \"):]\n",
    "                else:\n",
    "                    citation_nums = \"0\"\n",
    "            else:\n",
    "                citation_nums = \"0\"\n",
    "    \n",
    "            for ai in range(len(author_ids)):\n",
    "                author_list[author_ids[ai]].append(title)\n",
    "                datas.append({\n",
    "                    \"title\": title,\n",
    "                    \"author_id\": author_ids[ai],\n",
    "                    \"author_names\": author_names[ai],\n",
    "                    \"paper_year\": int(paper_year) if paper_year else None,\n",
    "                    \"citation_nums\": int(citation_nums)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(\"In paper error \", e)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for i in range(100):\n",
    "    rdns = sample_hap()\n",
    "    k = rdns[0]\n",
    "    n = rdns[1]\n",
    "\n",
    "    indexes = random.sample([b for b in range(50)], k=15)\n",
    "    for idx in indexes:\n",
    "        searched_combs.append(f\"{k}_{n}_{idx}\")\n",
    "        pn = idx*10\n",
    "        try:\n",
    "            drv.get(f\"https://scholar.google.com/scholar?start={pn}?hl=en&as_sdt=0%2C5&as_ylo=2020&q={k}+{n}&btnG=\")\n",
    "            time.sleep(5.0)\n",
    "            \n",
    "            html = drv.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            assert \"solving the above CAPTCHA\" not in soup.text\n",
    "        except Exception as e:\n",
    "            print(\"error \", e)\n",
    "            time.sleep(300.0)\n",
    "            continue\n",
    "        is_ok = get_titles_and_author_ids(soup)\n",
    "        print(\"Data lens : \", len(datas))\n",
    "        df = pd.DataFrame(datas)\n",
    "        df.to_csv(\"output.csv\", index=False)\n",
    "        sdf = pd.DataFrame(searched_combs)\n",
    "        sdf.to_csv(\"searched_combs.csv\", index=False)\n",
    "        if not is_ok:\n",
    "            time.sleep(300.0)\n",
    "        time.sleep(15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d815ce-708d-4f5f-86ce-6cdcd9a91db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = soup.find_all(\"div\", class_=re.compile(\"gs_scl\"))\n",
    "print(len(papers))\n",
    "for paper in papers:\n",
    "    title = paper.find(\"h3\", class_=re.compile(\"gs_rt\")).text\n",
    "    author_ids = [re.sub('&hl=en&oi=sra', '', a['href']) for a in paper.find(\"div\", class_=re.compile(\"gs_a\")).find_all(\"a\")]\n",
    "    print(title, author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d962698-7ccd-43be-a9a9-6162b81ded5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa83b0-27ae-44fb-a817-f5cc6af3763e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee816e-daec-47c0-8bbc-e2067d0cfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pds = pd.read_csv(\"output.csv\")\n",
    "pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703bb2b-6bf5-4386-a341-7dabb5b05a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc13de3-3457-4b21-8c01-095dcb772d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee265c-1c6a-4181-a835-c28c6b42d9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62efef-8c85-4925-8d63-ba4e85d31a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb944cb-8cde-4499-986c-fefd682783a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeeee93-1cc4-4768-843b-c4bc2f28c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "  \"engine\": \"google_scholar_author\",\n",
    "  \"author_id\": \"P2oSOR0AAAAJ\",\n",
    "  \"api_key\": \"\",\n",
    "    \"sort\": \"pub_date\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "author = results[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3a7d0-f9f7-4474-b87a-12a8ad1466c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "API_KEY = \"\"\n",
    "author_ids = ['FDG3_JMAAAAJ', 'tEyBgFQAAAAJ', '-ZJaGikAAAAJ', 'N2S3jFcAAAAJ', 'eGj3ay4AAAAJ']\n",
    "\n",
    "params = {\n",
    "    \"engine\": \"google_scholar_author\",\n",
    "    \"author_id\": 'tEyBgFQAAAAJ',\n",
    "    \"hl\": \"en\",\n",
    "    \"sort\": \"pubdate\",  # pub_date 대신 pubdate (docs 기준)\n",
    "    \"start\": 0,\n",
    "    \"api_key\": API_KEY,\n",
    "}\n",
    "\n",
    "resp = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "data = resp.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9db1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb2507-1fa2-44ef-b0d8-0c38ffd2e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google_scholar\"\n",
    "\n",
    "datas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92322abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in range(400):\n",
    "    rdns = sample_hap()\n",
    "    k = rdns[0]\n",
    "    n = rdns[1]\n",
    "    indexes = random.sample([b for b in range(100)], k=5)\n",
    "\n",
    "    for page in indexes:\n",
    "        try:\n",
    "            params = {\n",
    "                \"api_key\": api_key,\n",
    "                \"query\": f\"{k} {n}\",\n",
    "                \"results\": \"20\",\n",
    "                \"page\": page,\n",
    "                \"language\": \"en\",\n",
    "                \"lr\": \"lang_en\",\n",
    "                \"as_ylo\": \"2019\",\n",
    "                \"as_yhi\": \"2026\",\n",
    "                \"scisbd\": False\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "            for sc in data['scholar_results']:\n",
    "                try:\n",
    "                    title = sc['title']\n",
    "                    author_ids = [a['author_id'] for a in sc['authors']]\n",
    "                    author_names = [a['name'] for a in sc['authors']]\n",
    "                    matches = re.findall(r\"20\\d{2}\", sc['displayed_link'])\n",
    "                    paper_year = matches[-1]\n",
    "\n",
    "                    citation_nums = sc['inline_links']['cited_by']['total']\n",
    "                    if \"Related\" not in citation_nums:\n",
    "                        citation_nums = citation_nums[len(\"Cited by \"):]\n",
    "                    else:\n",
    "                        citation_nums = \"0\"\n",
    "\n",
    "                    for ai in range(len(author_ids)):\n",
    "                        datas.append({\n",
    "                            \"title\": title,\n",
    "                            \"author_id\": author_ids[ai],\n",
    "                            \"author_names\": author_names[ai],\n",
    "                            \"paper_year\": int(paper_year) if paper_year else None,\n",
    "                            \"citation_nums\": int(citation_nums)\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"[{k} {n} {page}] In paper error \", e)\n",
    "                    continue\n",
    "            \n",
    "            print(f\"[{k} {n} {page}] Data lens : \", len(datas))\n",
    "            df = pd.DataFrame(datas)\n",
    "            df.to_csv(\"output_onlydog.csv\", index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"[{k} {n} {page}] In Out error \", e)\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaff98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6d47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87264ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google_scholar/profiles\"\n",
    "# url = \"https://api.scrapingdog.com/google_scholar\"\n",
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    \"mauthors\": \"snu\",\n",
    "    \"results\": \"20\",\n",
    "    \"page\": 1,\n",
    "    \"language\": \"en\",\n",
    "    \"lr\": \"lang_en\",\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "# params = {\n",
    "#     \"api_key\": api_key,\n",
    "#     \"mauthors\": \"snu\"\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65c743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = [\n",
    "    \"Kang\", \"Ko\", \"Gwak\", \"Gu\", \"Guk\", \"Kwon\", \"Keum\", \"Ki\",\n",
    "    \"Na\", \"Nam\", \"Namgung\", \"Noh\", \"Non\", \"Dan\", \"Dam\", \"Dang\",\n",
    "    \"Do\", \"Dokgo\", \"Dongbang\", \"Dong\", \"Du\", \"Ra\", \"Ryeong\",\n",
    "    \"Ryu\", \"Ryuk\", \"Ri\",\n",
    "    \"Ma\", \"Man\", \"Maeng\", \"Myeong\", \"Mo\", \"Mok\", \"Muk\", \"Moon\",\n",
    "    \"Min\", \"Park\", \"Ban\", \"Bang\", \"Bae\", \"Baek\", \"Beom\", \"Byun\",\n",
    "    \"Bok\", \"Bong\", \"Boo\", \"Bi\", \"Bin\",\n",
    "    \"Sa\", \"Sam\", \"Sang\", \"Seo\", \"Seomun\", \"Seon\", \"Seonwoo\", \"Sung\",\n",
    "    \"So\", \"Son\", \"Song\", \"Su\", \"Seung\", \"Si\", \"Shin\", \"Sim\",\n",
    "    \"A\", \"Ahn\", \"Ae\", \"Yang\", \"Eo\", \"Eom\", \"Yeo\", \"Yeon\", \"Yeom\",\n",
    "    \"Young\", \"Ye\", \"Oh\", \"Ok\", \"On\", \"Ong\", \"Wang\", \"Yo\", \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Wi\", \"Yu\", \"Yuk\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jeol\", \"Jung\", \"Je\", \"Jegal\",\n",
    "    \"Jo\", \"Jwa\", \"Joo\", \"Juk\", \"Jun\", \"Ji\", \"Jin\",\n",
    "    \"Cha\", \"Chae\", \"Cheo\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Chu\", \"Tak\", \"Tan\", \"Tang\", \"Tae\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\", \"Hyun\", \"Hyeong\", \"Ho\",\n",
    "    \"Hong\", \"Hwa\", \"Hwang\", \"Hwangbo\", \"Hu\", \"Heung\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_onlydog.csv\")\n",
    "print(len(df))\n",
    "\n",
    "pattern = \"|\".join(surnames)\n",
    "\n",
    "duplicated_title = df[df.duplicated(subset=['title'])]\n",
    "print(len(duplicated_title))\n",
    "\n",
    "duplicated = df[df.duplicated(subset=['author_id'])]\n",
    "print(len(duplicated))\n",
    "\n",
    "print(len(df))\n",
    "filtered = duplicated[duplicated[\"author_names\"].str.contains(pattern, case=False, na=False)]\n",
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052f3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ea345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google_scholar/author\"\n",
    "\n",
    "detail_profiles = []\n",
    "extracted_authors = []\n",
    "\n",
    "for aidx, author_id in tqdm(enumerate(author_ids)):\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"author_id\": author_id,\n",
    "        \"page\": 0,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    author = data['author']\n",
    "\n",
    "    input_data = {\n",
    "        \"author_id\": \"author_id\",\n",
    "        \"name\": author['name'],\n",
    "        \"affiliations\": author['affiliations'],\n",
    "        \"email\": author['email'],\n",
    "        \"interests\": [a['title'] for a in author['interests']],\n",
    "        \"image_thumbnail\": author['thumbnail'],\n",
    "        \"articles\": [{\n",
    "            \"title\": d['title'],\n",
    "            \"citation_id\": d['citation_id'],\n",
    "            \"publication\": d['publication'],\n",
    "            \"citation_count\": d['cited_by']['value'],\n",
    "            \"year\": int(d['year']),\n",
    "        } for d in data['articles']],\n",
    "        \"total_citation_count\": data['cited_by']['table'][0]['citations']['all'],\n",
    "        \"since_2020_citation_count\": data['cited_by']['table'][0]['citations']['since_2020'],\n",
    "        \"h_index\": data['cited_by']['table'][1]['h_index']['all'],\n",
    "    }\n",
    "    detail_profiles.append(input_data)\n",
    "\n",
    "    if len(data['co_authors']) > 0:\n",
    "        for co_author in data['co_authors']:\n",
    "            author_input_data = {\n",
    "                \"author_id\": co_author['author_id'],\n",
    "                \"author_names\": co_author['name'],\n",
    "                \"affiliations\": co_author['affiliations'],\n",
    "            }\n",
    "            extracted_authors.append(author_input_data)\n",
    "    \n",
    "    if aidx % 50 == 49:\n",
    "        pd.DataFrame(detail_profiles).to_csv(f\"detail_profiles_{aidx}.csv\", index=False)\n",
    "        pd.DataFrame(extracted_authors).to_csv(f\"extracted_authors_{aidx}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d02fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad27bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    \"author_id\": \"Gz8lsIwAAAAJ\",\n",
    "    \"page\": 20,\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235f28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = [\n",
    "    \"Kang\", \"Ko\", \"Gwak\", \"Gu\", \"Guk\", \"Kwon\", \"Keum\", \"Ki\",\n",
    "    \"Na\", \"Nam\", \"Namgung\", \"Noh\", \"Non\", \"Dan\", \"Dam\", \"Dang\",\n",
    "    \"Do\", \"Dokgo\", \"Dongbang\", \"Dong\", \"Du\", \"Ra\", \"Ryeong\",\n",
    "    \"Ryu\", \"Ryuk\", \"Ri\",\n",
    "    \"Ma\", \"Man\", \"Maeng\", \"Myeong\", \"Mo\", \"Mok\", \"Muk\", \"Moon\",\n",
    "    \"Min\", \"Park\", \"Ban\", \"Bang\", \"Bae\", \"Baek\", \"Beom\", \"Byun\",\n",
    "    \"Bok\", \"Bong\", \"Boo\", \"Bi\", \"Bin\",\n",
    "    \"Sa\", \"Sam\", \"Sang\", \"Seo\", \"Seomun\", \"Seon\", \"Seonwoo\", \"Sung\",\n",
    "    \"So\", \"Son\", \"Song\", \"Su\", \"Seung\", \"Si\", \"Shin\", \"Sim\",\n",
    "    \"A\", \"Ahn\", \"Ae\", \"Yang\", \"Eo\", \"Eom\", \"Yeo\", \"Yeon\", \"Yeom\",\n",
    "    \"Young\", \"Ye\", \"Oh\", \"Ok\", \"On\", \"Ong\", \"Wang\", \"Yo\", \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Wi\", \"Yu\", \"Yuk\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jeol\", \"Jung\", \"Je\", \"Jegal\",\n",
    "    \"Jo\", \"Jwa\", \"Joo\", \"Juk\", \"Jun\", \"Ji\", \"Jin\",\n",
    "    \"Cha\", \"Chae\", \"Cheo\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Chu\", \"Tak\", \"Tan\", \"Tang\", \"Tae\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\", \"Hyun\", \"Hyeong\", \"Ho\",\n",
    "    \"Hong\", \"Hwa\", \"Hwang\", \"Hwangbo\", \"Hu\", \"Heung\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_onlydog.csv\")\n",
    "print(len(df))\n",
    "\n",
    "pattern = \"|\".join(surnames)\n",
    "\n",
    "duplicated_title = df[df.duplicated(subset=['title'])]\n",
    "print(len(duplicated_title))\n",
    "\n",
    "duplicated = df[df.duplicated(subset=['author_id'])]\n",
    "print(len(duplicated))\n",
    "\n",
    "filtered = duplicated[duplicated[\"author_names\"].str.contains(pattern, case=False, na=False)]\n",
    "print(len(filtered))\n",
    "\n",
    "author_ids = filtered['author_id'].tolist()\n",
    "\n",
    "def save_data(aidx, detail_profiles, extracted_authors):\n",
    "    pd.DataFrame(detail_profiles).to_csv(f\"detail_profiles.csv\", index=False)\n",
    "    pd.DataFrame(extracted_authors).to_csv(f\"extracted_authors.csv\", index=False)\n",
    "\n",
    "    with open(f\"detail_profiles.json\", \"w\") as f:\n",
    "        json.dump(detail_profiles, f, indent=4)\n",
    "\n",
    "    with open(f\"extracted_authors.json\", \"w\") as f:\n",
    "        json.dump(extracted_authors, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved {len(detail_profiles)} detail_profiles and {len(extracted_authors)} extracted_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google_scholar/author\"\n",
    "\n",
    "detail_profiles = []\n",
    "extracted_authors = []\n",
    "used_author_ids = []\n",
    "\n",
    "for aidx, author_id in tqdm(enumerate(author_ids[:500])):\n",
    "    # -------- 1) 첫 페이지 요청 (page = 0) --------\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"author_id\": author_id,\n",
    "        \"page\": 0,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"[author {author_id}] Request failed with status code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    data = response.json()\n",
    "    first_data = data  # co_authors, cited_by, author 정보는 첫 페이지 기준으로 사용\n",
    "    all_articles = data.get(\"articles\", []).copy()\n",
    "\n",
    "    # -------- 2) articles 길이가 20이면 page를 20씩 늘려가며 추가 수집 --------\n",
    "    # page: 20, 40, 60, 80, 100 까지 시도\n",
    "    if len(data.get(\"articles\", [])) == 20:\n",
    "        for page in range(20, 101, 20):\n",
    "            params = {\n",
    "                \"api_key\": api_key,\n",
    "                \"author_id\": author_id,\n",
    "                \"page\": page,\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"[author {author_id}] (page {page}) Request failed with status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            page_data = response.json()\n",
    "            page_articles = page_data.get(\"articles\", [])\n",
    "\n",
    "            # articles 말고는 아무것도 안 쓰고, 기존 all_articles에만 이어붙임\n",
    "            if not page_articles:\n",
    "                # 더 이상 가져올 게 없으면 중단\n",
    "                break\n",
    "\n",
    "            all_articles.extend(page_articles)\n",
    "\n",
    "            # 이 페이지에서 20개 미만이면 더 이상 다음 page 안 감\n",
    "            if len(page_articles) < 20:\n",
    "                break\n",
    "\n",
    "    # -------- 3) 첫 페이지의 author / citation / co_author + 모든 articles로 최종 구조 생성 --------\n",
    "    author = first_data[\"author\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"author_id\": author_id,  # 원래 코드에 \"author_id\" 문자열이 들어가 있던 부분 수정\n",
    "        \"name\": author.get(\"name\"),\n",
    "        \"affiliations\": author.get(\"affiliations\"),\n",
    "        \"email\": author.get(\"email\"),\n",
    "        \"interests\": [a[\"title\"] for a in author.get(\"interests\", [])],\n",
    "        \"image_thumbnail\": author.get(\"thumbnail\"),\n",
    "        \"articles\": [\n",
    "            {\n",
    "                \"title\": d.get(\"title\"),\n",
    "                \"citation_id\": d.get(\"citation_id\"),\n",
    "                \"publication\": d.get(\"publication\"),\n",
    "                \"citation_count\": d.get(\"cited_by\", {}).get(\"value\"),\n",
    "                # year가 없을 수도 있으니 방어코드 (원래처럼 int()만 쓰고 싶으면 아래 두 줄을 d[\"year\"]로 바꿔도 됨)\n",
    "                \"year\": int(d[\"year\"]) if d.get(\"year\") not in (None, \"\") else None,\n",
    "            }\n",
    "            for d in all_articles if d.get(\"title\") != \"\"\n",
    "        ],\n",
    "        \"total_citation_count\": first_data[\"cited_by\"][\"table\"][0][\"citations\"][\"all\"],\n",
    "        \"since_2020_citation_count\": first_data[\"cited_by\"][\"table\"][0][\"citations\"][\"since_2020\"],\n",
    "        \"h_index\": first_data[\"cited_by\"][\"table\"][1][\"h_index\"][\"all\"],\n",
    "    }\n",
    "    detail_profiles.append(input_data)\n",
    "\n",
    "    # co_authors도 첫 페이지 기준으로만 읽음\n",
    "    co_authors = first_data.get(\"co_authors\", [])\n",
    "    if co_authors:\n",
    "        for co_author in co_authors:\n",
    "            author_input_data = {\n",
    "                \"author_id\": co_author.get(\"author_id\"),\n",
    "                \"author_names\": co_author.get(\"name\"),\n",
    "                \"affiliations\": co_author.get(\"affiliations\"),\n",
    "            }\n",
    "            extracted_authors.append(author_input_data)\n",
    "\n",
    "    # -------- 4) 주기적으로 csv 저장 --------\n",
    "    if aidx % 50 == 49:\n",
    "        save_data(aidx, detail_profiles, extracted_authors)\n",
    "\n",
    "save_data(aidx, detail_profiles, extracted_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deeab30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef41f24c",
   "metadata": {},
   "source": [
    "## Google 검색으로 Google scholar profile 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google\"\n",
    "\n",
    "unis = {\n",
    "    'korea.ac.kr': 1, 'yonsei': 1, 'snu': 2, 'kaist': 2, 'postech': 1\n",
    "}\n",
    "keywords = [\"deep\", \"machine\", \"artificial intelligence\", \"computer\"] # 각 3000명씩\n",
    "\n",
    "datas = []\n",
    "for uni in unis:\n",
    "    for keyword in keywords:\n",
    "        for idx in tqdm(range(1, int(300 * unis[uni]))):\n",
    "            params = {\n",
    "                \"api_key\": api_key,\n",
    "                \"query\": f\"{uni} {keyword} site:https://scholar.google.com/citations\",\n",
    "                \"country\": \"kr\",\n",
    "                \"page\": f\"{idx}\",\n",
    "                \"advance_search\": \"true\",\n",
    "                \"domain\": \"google.com\"\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "            else:\n",
    "                print(f\"[{idx}] - Request failed with status code: {response.status_code}\")\n",
    "\n",
    "            try:\n",
    "                for d in data['organic_results']:\n",
    "                    name = d['title']\n",
    "                    link = d['link']\n",
    "                    author_id = link.split('/citations?user=')[1].split('&')[0]\n",
    "                    snippet = d['snippet']\n",
    "\n",
    "                    input_data = {\n",
    "                        'name': name,\n",
    "                        'author_id': author_id,\n",
    "                        'snippet': snippet,\n",
    "                        'link': link,\n",
    "                        'source': d['source']\n",
    "                    }\n",
    "                    datas.append(input_data)\n",
    "                \n",
    "                if idx == 1:\n",
    "                    print(f\"\\nStart for total_results: {data['search_information']['total_results']}\\n\\n\")\n",
    "                if data['search_information']['total_results'] < idx*10:\n",
    "                    print(f\"\\n\\nEnd for total_results: {data['search_information']['total_results']}\\n\\n\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        pd.DataFrame(datas).to_csv('scholar_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "767da6c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (1060610328.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 48\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_key = \"\"\n",
    "url = \"https://api.scrapingdog.com/google\"\n",
    "\n",
    "idx = 1\n",
    "uni = \"MIT\"\n",
    "keyword = \"deep\"\n",
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    \"query\": f\"{uni} {keyword} site:https://scholar.google.com/citations\",\n",
    "    \"country\": \"kr\",\n",
    "    \"page\": f\"{idx}\",\n",
    "    \"advance_search\": \"true\",\n",
    "    \"domain\": \"google.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"[{idx}] - Request failed with status code: {response.status_code}\")\n",
    "\n",
    "try:\n",
    "    for d in data['organic_results']:\n",
    "        name = d['title']\n",
    "        link = d['link']\n",
    "        author_id = link.split('/citations?user=')[1].split('&')[0]\n",
    "        snippet = d['snippet']\n",
    "\n",
    "        input_data = {\n",
    "            'name': name,\n",
    "            'author_id': author_id,\n",
    "            'snippet': snippet,\n",
    "            'link': link,\n",
    "            'source': d['source']\n",
    "        }\n",
    "        datas.append(input_data)\n",
    "    \n",
    "    if idx == 1:\n",
    "        print(f\"\\nStart for total_results: {data['search_information']['total_results']}\\n\\n\")\n",
    "    if data['search_information']['total_results'] < idx*10:\n",
    "        print(f\"\\n\\nEnd for total_results: {data['search_information']['total_results']}\\n\\n\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    continue\n",
    "\n",
    "pd.DataFrame(datas).to_csv('scholar_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b435812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
