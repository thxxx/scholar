{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfae9ce-b22f-4024-8988-f6deafba0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from utils import upload_to_huggingface, build_driver, grok_api_key\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=grok_api_key,\n",
    "    base_url=\"https://api.x.ai/v1/\",\n",
    ")\n",
    "\n",
    "def return_text(soup):\n",
    "    NOISE_ID_RE = re.compile(\n",
    "        r\"^(actionbar|jp-|jetpack|wpgroho|grofiles|wp-emoji|comment-|bilmur|follow-bubble)\",\n",
    "        re.I,\n",
    "    )\n",
    "\n",
    "    GENERIC_TAGS = [\n",
    "        \"div\", \"section\", \"article\", \"main\",\n",
    "        \"header\", \"footer\", \"nav\", \"span\", \"p\"\n",
    "    ]\n",
    "\n",
    "    # Common \"noisy\" attributes added by frameworks/trackers/renderers\n",
    "    DROP_ATTR_PREFIXES = (\"data-\", \"aria-\")\n",
    "    DROP_ATTR_NAMES = {\n",
    "        \"role\", \"rel\", \"target\", \"tabindex\",\n",
    "        # \"itemprop\", \"itemscope\", \"itemtype\",\n",
    "        \"contenteditable\", \"draggable\", \"spellcheck\",\n",
    "        \"loading\", \"decoding\", \"fetchpriority\",\n",
    "        \"srcset\", \"sizes\", \"integrity\", \"crossorigin\",\n",
    "        \"referrerpolicy\", \"nonce\",\n",
    "        \"width\", \"height\",\n",
    "    }\n",
    "    ON_EVENT_ATTR_RE = re.compile(r\"^on[a-z]+$\", re.I)\n",
    "\n",
    "    def strip_noisy_nodes_and_attrs(root):\n",
    "        # Remove comments\n",
    "        for c in root.find_all(string=lambda s: isinstance(s, Comment)):\n",
    "            c.extract()\n",
    "\n",
    "        # Remove whole noisy tags\n",
    "        for t in root.find_all([\"script\", \"style\", \"noscript\", \"template\", \"svg\", \"canvas\"]):\n",
    "            t.decompose()\n",
    "\n",
    "        # Remove noisy attributes\n",
    "        for t in root.find_all(True):\n",
    "            attrs = list(t.attrs.keys())\n",
    "            for k in attrs:\n",
    "                kl = k.lower()\n",
    "\n",
    "                # Drop inline event handlers like onclick/onload/...\n",
    "                if ON_EVENT_ATTR_RE.match(kl):\n",
    "                    t.attrs.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "                # Drop data-*, aria-*\n",
    "                if any(kl.startswith(p) for p in DROP_ATTR_PREFIXES):\n",
    "                    t.attrs.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "                # Drop known noisy attributes\n",
    "                if kl in DROP_ATTR_NAMES:\n",
    "                    t.attrs.pop(k, None)\n",
    "                    continue\n",
    "\n",
    "    def remove_class_style_regex(html: str) -> str:\n",
    "        # Remove class/style in both quote styles\n",
    "        html = re.sub(r\"\\sclass=\\\"[^\\\"]*\\\"\", \"\", html)\n",
    "        html = re.sub(r\"\\sclass='[^']*'\", \"\", html)\n",
    "        html = re.sub(r\"\\sstyle=\\\"[^\\\"]*\\\"\", \"\", html)\n",
    "        html = re.sub(r\"\\sstyle='[^']*'\", \"\", html)\n",
    "        return html\n",
    "\n",
    "    def shrink_html_tokens(html) -> str:\n",
    "        # First pass: remove noisy nodes/attrs (framework/tracker bloat)\n",
    "        strip_noisy_nodes_and_attrs(html)\n",
    "\n",
    "        # Second pass: remove Jetpack/WordPress-ish noisy blocks by id/class pattern\n",
    "        for t in list(html.find_all(True)):\n",
    "            if t is None:\n",
    "                continue\n",
    "\n",
    "            tid = t.get(\"id\") or \"\"\n",
    "            if tid and NOISE_ID_RE.match(tid):\n",
    "                t.decompose()\n",
    "                continue\n",
    "\n",
    "            classes = \" \".join(t.get(\"class\", [])) if t.get(\"class\") else \"\"\n",
    "            if classes and NOISE_ID_RE.search(classes):\n",
    "                t.decompose()\n",
    "                continue\n",
    "\n",
    "        s = str(html)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        s = s.replace(\"\\t\", \"\")\n",
    "        s = re.sub(r\"\\n+\", \"\\n\", s)\n",
    "        s = re.sub(r\"[ ]{2,}\", \" \", s)\n",
    "\n",
    "        # Collapse generic tags\n",
    "        for tag in GENERIC_TAGS:\n",
    "            s = re.sub(fr\"<{tag}(\\s[^>]*)?>\", \"<>\", s)\n",
    "            s = re.sub(fr\"</{tag}>\", \"</>\", s)\n",
    "\n",
    "        # Collapse empty generic pairs\n",
    "        s = re.sub(r\"<>\\s*</>\", \"<>\", s)\n",
    "\n",
    "        # Remove inter-tag whitespace\n",
    "        s = re.sub(r\">\\s+<\", \"><\", s)\n",
    "\n",
    "        return s.strip()\n",
    "\n",
    "    # Usage\n",
    "    body = soup.find(\"body\")\n",
    "    sp = shrink_html_tokens(body)\n",
    "    sp = remove_class_style_regex(sp)\n",
    "    \n",
    "    return sp\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an information extraction assistant.\n",
    "\n",
    "Below is a raw HTML document of a person's profile website.\n",
    "Your task is to extract the following attributes **only if they are explicitly present in the HTML**.\n",
    "Do NOT infer, guess, or hallucinate any information.\n",
    "\n",
    "If an attribute cannot be found with reasonable certainty, return `None` for that field.\n",
    "\n",
    "### Attributes to extract\n",
    "- email: A contact email address of the owner found in the HTML (e.g., mailto links or visible text). only the owner's email address, not other people.\n",
    "- related_links: a list of links that appear to be related to the exact person. (e.g. linkedin, github, cv, blog, sns, etc.). Do not include every single paper, citation, project, labs or company links.\n",
    "- bio: A short biography or description text describing the person, company, or project. Write it in the first person. At leat 3 sentences, at most 6 sentences.\n",
    "e.g. I am the researcher who is interested in ...\n",
    "- page_type: If current html is not a profile page, return the type of the page. e.g. \"company\", \"blog\", \"other\", \"labs\", etc.\n",
    "If is not a profile page, return None for all the other attributes.\n",
    "- company_experiences: A list of company experiences of the owner. include the company name, title(Role), start date, end date.\n",
    "ex) \n",
    "\"company_experiences\": [\n",
    "    {\n",
    "      \"company_name\": \"Company A\",\n",
    "      \"title\": \"Reseach Scientist, TTS end LLM optimization\",\n",
    "      \"start_date\": \"2020-07\",\n",
    "      \"end_date\": \"2021-04\"\n",
    "    },{\n",
    "      \"company_name\": \"Los University\",\n",
    "      \"title\": \"Assistant professor\",\n",
    "      \"start_date\": null,\n",
    "      \"end_date\": null\n",
    "    }, etc\n",
    "]\n",
    "- education: A list of education experiences of the owner. include the school name, degree, start date, end date. only include BS, MS, PhD. no \n",
    "ex)\n",
    "\"education\": [\n",
    "    {\n",
    "      \"school_name\": \"School A\",\n",
    "      \"degree\": \"Ph.D. in Computer Science\",\n",
    "      \"start_date\": \"2018-11\",\n",
    "      \"end_date\": \"2022-01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "Write date in format \"YYYY-MM\"\n",
    "\n",
    "### Output format\n",
    "Return a **valid JSON object** exactly in the following format:\n",
    "\n",
    "{\n",
    "  \"email\": string | None,\n",
    "  \"related_links\": list[string] | None,\n",
    "  \"bio\": string | None,\n",
    "  \"page_type\": string,\n",
    "  \"company_experiences\": list[dict] | None,\n",
    "  \"education\": list[dict] | None,\n",
    "}\n",
    "\n",
    "### Rules\n",
    "- Use `None` (not null, not empty string) if the value is missing\n",
    "- For `related_links`, return `None` if no relevant links are found\n",
    "- Do not include duplicate links\n",
    "- Do not include navigation-only or irrelevant links (e.g., privacy policy, terms)\n",
    "- Preserve the original text as-is (do not paraphrase)\n",
    "- Do not add any explanations or extra text outside the JSON\n",
    "- Only include in related_links of a list of links that appear to be related to the exact person. (e.g. linkedin, github, cv, blog, sns, etc.). Do not include every single paper, citation, project, labs or company links.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "def calc_cost_with_cache(\n",
    "    usage,\n",
    "    input_price_per_1k,\n",
    "    output_price_per_1k,\n",
    "    input_price_per_cached_1k,\n",
    "):\n",
    "    # prompt\n",
    "    total_prompt = usage.prompt_tokens\n",
    "    cached = usage.prompt_tokens_details.cached_tokens or 0\n",
    "    billable_prompt = max(total_prompt - cached, 0)\n",
    "\n",
    "    # completion\n",
    "    completion = usage.completion_tokens\n",
    "\n",
    "    input_cost = billable_prompt / 1000 * input_price_per_1k + cached / 1000 * input_price_per_cached_1k\n",
    "    output_cost = completion / 1000 * output_price_per_1k\n",
    "\n",
    "    return {\n",
    "        \"prompt_tokens_total\": total_prompt,\n",
    "        \"prompt_tokens_cached\": cached,\n",
    "        \"prompt_tokens_billable\": billable_prompt,\n",
    "        \"completion_tokens\": completion,\n",
    "        \"input_cost_usd\": input_cost,\n",
    "        \"output_cost_usd\": output_cost,\n",
    "        \"total_cost_usd\": input_cost + output_cost,\n",
    "    }\n",
    "\n",
    "\n",
    "PRICES = {\n",
    "    \"input\": 0.0002,\n",
    "    \"output\": 0.0005,\n",
    "    \"cached\": 0.00005,\n",
    "}\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def now_str():\n",
    "    return datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "\n",
    "def safe_str(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def default_output(home_link: str):\n",
    "    return {\n",
    "        \"email\": None,\n",
    "        \"related_links\": [home_link] if home_link else [],\n",
    "        \"bio\": None,\n",
    "        \"page_type\": None,\n",
    "        \"company_experiences\": [],\n",
    "        \"education\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_json_from_llm(text: str):\n",
    "    \"\"\"\n",
    "    LLM이 JSON만 주면 그대로 파싱.\n",
    "    JSON 앞뒤로 설명이 붙는 경우도 많아서, 가장 바깥 { ... } 혹은 [ ... ] 블록을 찾아 파싱 시도.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        raise ValueError(\"LLM output is None\")\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    fenced = re.search(r\"```(?:json)?\\s*(\\{.*\\}|\\[.*\\])\\s*```\", text, re.DOTALL)\n",
    "    if fenced:\n",
    "        return json.loads(fenced.group(1))\n",
    "\n",
    "    brace = re.search(r\"(\\{.*\\})\", text, re.DOTALL)\n",
    "    if brace:\n",
    "        return json.loads(brace.group(1))\n",
    "\n",
    "    bracket = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if bracket:\n",
    "        return json.loads(bracket.group(1))\n",
    "\n",
    "    raise ValueError(f\"Could not parse JSON from LLM output. Head={text[:120]!r}\")\n",
    "\n",
    "\n",
    "def normalize_output(obj, home_link: str):\n",
    "    if isinstance(obj, str):\n",
    "        obj = extract_json_from_llm(obj)\n",
    "\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(f\"Parsed output is not dict: {type(obj)}\")\n",
    "\n",
    "    out = default_output(home_link)\n",
    "\n",
    "    # merge\n",
    "    for k in out.keys():\n",
    "        if k in obj:\n",
    "            out[k] = obj[k]\n",
    "\n",
    "    # 타입 보정\n",
    "    if out[\"email\"] == \"\" or out[\"email\"] == \"None\":\n",
    "        out[\"email\"] = None\n",
    "\n",
    "    if out[\"bio\"] == \"\" or out[\"bio\"] == \"None\":\n",
    "        out[\"bio\"] = None\n",
    "\n",
    "    if out[\"page_type\"] == \"\" or out[\"page_type\"] == \"None\":\n",
    "        out[\"page_type\"] = None\n",
    "\n",
    "    if out[\"related_links\"] is None:\n",
    "        out[\"related_links\"] = []\n",
    "    elif isinstance(out[\"related_links\"], str):\n",
    "        out[\"related_links\"] = [out[\"related_links\"]]\n",
    "\n",
    "    if out[\"company_experiences\"] is None:\n",
    "        out[\"company_experiences\"] = []\n",
    "    if out[\"education\"] is None:\n",
    "        out[\"education\"] = []\n",
    "\n",
    "    # home_link는 최소 포함\n",
    "    if home_link and home_link not in out[\"related_links\"]:\n",
    "        out[\"related_links\"].append(home_link)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def dump_jsonl(path, rows):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095373b-2c45-4cb8-b57b-b19c713d6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"most_recent_total_detail_profiles_with_homepage.csv\")\n",
    "print(len(df))\n",
    "\n",
    "drv = build_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb148b-399a-4d41-8a5b-8f6dff6ffd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Main loop (completed)\n",
    "# ----------------------------\n",
    "total_dollars = 0.0\n",
    "results = []          # 성공/실패 포함 전체 로그\n",
    "success_rows = []     # 파싱 성공 결과만 별도\n",
    "failed_rows = []      # 실패 로그만 별도\n",
    "\n",
    "# 중간 저장 파일들\n",
    "run_id = now_str()\n",
    "out_dir = \"outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac0ac6-db4e-41cc-a84f-fdeeff52b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f\"extracted_home_info_total.csv\"\n",
    "\n",
    "done_ids = []\n",
    "if os.path.exists(csv_path):\n",
    "    odf = pd.read_csv(csv_path)\n",
    "    success_rows = odf.to_dict(orient=\"records\")\n",
    "    done_ids = set(odf[\"author_id\"].tolist())\n",
    "    print(f\"Loaded {len(done_ids)} done ids\")\n",
    "\n",
    "# 비용 로그\n",
    "cost_logs = []\n",
    "\n",
    "base_prompt = prompt  # 너가 이미 만들어둔 prompt를 base로 둔다고 가정\n",
    "\n",
    "SAVE_EVERY = 50\n",
    "PAGE_LOAD_TIMEOUT_SEC = 30\n",
    "AFTER_GET_SLEEP_SEC = 2\n",
    "RETRY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43286837-8b09-4d2d-8b4c-d41e342d68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "starti = 9606\n",
    "endi = int(len(df)//2)\n",
    "\n",
    "print(f\"Start from {len(success_rows)} datas\")\n",
    "for i in tqdm(range(starti, endi)):\n",
    "\n",
    "    data = df.iloc[i]\n",
    "    if data.get(\"author_id\", \"\") in done_ids:\n",
    "        print(f\"[{i}th] - 중복\")\n",
    "        continue\n",
    "\n",
    "    hl = data.get(\"home_link\", None)\n",
    "    if pd.isna(hl) or not safe_str(hl).strip():\n",
    "        continue\n",
    "\n",
    "    url = safe_str(hl).strip()\n",
    "    print(f\"[{i}th] Check url : {url}\")\n",
    "\n",
    "    row_meta = {\n",
    "        \"row_index\": int(i),\n",
    "        \"author_id\": safe_str(data.get(\"author_id\", \"\")),\n",
    "        \"name\": safe_str(data.get(\"name\", \"\")),\n",
    "        \"home_link\": url,\n",
    "    }\n",
    "    # LinkedIn은 스킵/별도 처리\n",
    "    if \"linkedin.com\" in url:\n",
    "        print(f\"[{i}th] - SAVED ON LINKEDIN\")\n",
    "        output_dict = default_output(url)\n",
    "        output_dict.update({\n",
    "            \"page_type\": \"linkedin\",\n",
    "        })\n",
    "\n",
    "        success_rows.append({**row_meta, **output_dict, \"ok\": True})\n",
    "        \n",
    "        # --- periodic save ---\n",
    "        if len(success_rows) % SAVE_EVERY == SAVE_EVERY - 1:\n",
    "            pd.DataFrame(success_rows).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "            upload_to_huggingface(csv_path)\n",
    "            print(f\"\\n\\nSAVED in LINKEDIN! {len(success_rows)}\\n\\n\")\n",
    "\n",
    "        continue\n",
    "\n",
    "    # --- Fetch + extract with retries ---\n",
    "    last_err = None\n",
    "    extracted_text = None\n",
    "\n",
    "    for attempt in range(RETRY + 1):\n",
    "        try:\n",
    "            drv.get(url)\n",
    "            time.sleep(AFTER_GET_SLEEP_SEC)\n",
    "\n",
    "            # body 태그 등장 대기\n",
    "            WebDriverWait(drv, PAGE_LOAD_TIMEOUT_SEC).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            html = drv.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            extracted_text = return_text(soup)  # 이미 있다고 가정\n",
    "            if not extracted_text or len(extracted_text.strip()) < 50:\n",
    "                raise ValueError(\"Extracted text too short (maybe blocked / empty page).\")\n",
    "            print(f\"[{i}th] 링크로부터 HTML 읽어오기 성 공\")\n",
    "\n",
    "            break  # 성공\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"[{i}th] Parsing Error \", e)\n",
    "            if attempt < RETRY:\n",
    "                time.sleep(1.5 * (attempt + 1))\n",
    "                continue\n",
    "\n",
    "    if extracted_text is None:\n",
    "        err_msg = f\"Failed to fetch/extract after retries\"\n",
    "        continue\n",
    "\n",
    "    # --- Build prompt per row ---\n",
    "    row_prompt = base_prompt + f\"\"\"\n",
    "### owner's name\n",
    "{safe_str(data.get('name', ''))}\n",
    "\n",
    "### HTML Document\n",
    "{extracted_text}\n",
    "\"\"\"\n",
    "\n",
    "    # --- LLM call + parse ---\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"grok-4-1-fast-non-reasoning\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a html extractor. Return ONLY valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": row_prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        raw_output = response.choices[0].message.content\n",
    "\n",
    "        # 비용 계산\n",
    "        try:\n",
    "            cost = calc_cost_with_cache(\n",
    "                response.usage,\n",
    "                PRICES[\"input\"],\n",
    "                PRICES[\"output\"],\n",
    "                PRICES[\"cached\"],\n",
    "            )\n",
    "            dollars = float(cost[\"total_cost_usd\"])\n",
    "        except Exception:\n",
    "            cost = None\n",
    "            dollars = 0.0\n",
    "        total_dollars += dollars\n",
    "        \n",
    "        # 출력 정규화\n",
    "        output_dict = normalize_output(raw_output, url)\n",
    "        \n",
    "        # tabular로도 저장하기 쉽게 flatten\n",
    "        success_rows.append({\n",
    "            **row_meta,\n",
    "            \"ok\": True,\n",
    "            \"email\": output_dict.get(\"email\"),\n",
    "            \"bio\": output_dict.get(\"bio\"),\n",
    "            \"page_type\": output_dict.get(\"page_type\"),\n",
    "            \"related_links\": json.dumps(output_dict.get(\"related_links\", []), ensure_ascii=False),\n",
    "            \"company_experiences\": json.dumps(output_dict.get(\"company_experiences\", []), ensure_ascii=False),\n",
    "            \"education\": json.dumps(output_dict.get(\"education\", []), ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "        print(f\"[{i}th] {row_meta['name']} -- [{total_dollars:.4f}]$ {output_dict} \\n\\n\")\n",
    "\n",
    "        # --- periodic save ---\n",
    "        if len(success_rows) % SAVE_EVERY == SAVE_EVERY - 1:\n",
    "            pd.DataFrame(success_rows).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "            upload_to_huggingface(csv_path)\n",
    "            print(f\"\\n\\nSAVED! {len(success_rows)}\\n\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"LLM Error \", e)\n",
    "        err_msg = f\"LLM/parse failed: {repr(e)}\"\n",
    "        continue\n",
    "\n",
    "# final save\n",
    "pd.DataFrame(success_rows).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "upload_to_huggingface(csv_path)\n",
    "\n",
    "print(f\"\\nDONE. success={len(success_rows)}\")\n",
    "print(f\"Total cost (USD): {total_dollars:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7671a-206a-4b01-be56-49fead6ba817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8e396-c126-4f3c-a785-743b6ad3272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69c5f3-fee4-4090-91a5-005714ddfda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b299a-8ea0-4ccb-991a-ba25e6d61c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
