{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc7be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "63516\n",
      "70349\n",
      "63516\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "prefix = \"linkedin_search_results\"\n",
    "pattern = f\"{prefix}*.csv\"\n",
    "csv_files = glob.glob(pattern)\n",
    "print(len(csv_files))\n",
    "\n",
    "dfs = []\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(e, path)\n",
    "\n",
    "df = pd.read_csv(\"linkedins_total_fourth.csv\")\n",
    "print(len(df))\n",
    "dfs.append(df)\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(len(merged))\n",
    "\n",
    "from_linkedin = merged[merged['link'].str.contains(\"linkedin.com/in/\")]\n",
    "\n",
    "pat = 'linkedin.com/in/'\n",
    "from_linkedin['linkedin_id'] = from_linkedin['link'].apply(lambda x: x[x.find(pat) + len(pat):].split(\"/\")[0])\n",
    "\n",
    "clean = from_linkedin.drop_duplicates(subset=[\"linkedin_id\"])\n",
    "print(len(clean))\n",
    "\n",
    "clean.to_csv(\"linkedins_total_fourthdqw.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f617edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>snippet</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>linkedin_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soohwan Lee 님 - Ph.D Candidate at UNIST | Huma...</td>\n",
       "      <td>I am a Ph.D. candidate in Design at UNIST, spe...</td>\n",
       "      <td>https://kr.linkedin.com/in/soohwanlee/ko</td>\n",
       "      <td>NaN</td>\n",
       "      <td>soohwanlee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yongjae Lee 님 - Associate Professor at UNIST |...</td>\n",
       "      <td>Associate Professor at UNIST | AI for Finance ...</td>\n",
       "      <td>https://kr.linkedin.com/in/yongjae-lee-5489821...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yongjae-lee-548982107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이건모 님 - AI 엔지니어 인턴 @ Allganize , 울산과학기술원(UNIST...</td>\n",
       "      <td>AI 엔지니어 인턴 @ Allganize , 울산과학기술원(UNIST) 산업공학과 ...</td>\n",
       "      <td>https://kr.linkedin.com/in/keonmo-lee/ko</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keonmo-lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jaeho Kim 님 - Ph.D in Time Series | Korea Univ...</td>\n",
       "      <td>Jaeho Kim is a Ph.D. Candidate at the Ulsan Na...</td>\n",
       "      <td>https://kr.linkedin.com/in/jaeho3690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jaeho3690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robotics &amp; AI Researcher |M.S. UNIST - Hyoungh...</td>\n",
       "      <td>UNIST ASL에서 M.S. Student를 진행 중이며, 자율주행 연구실CAIL...</td>\n",
       "      <td>https://kr.linkedin.com/in/hmue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hmue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70344</th>\n",
       "      <td>Yea Kang Yoon - Authentication AI/ML Product O...</td>\n",
       "      <td>Statistics. 2003 - 2006. Korea University Grap...</td>\n",
       "      <td>https://www.linkedin.com/in/yea-kang-yoon-3419415</td>\n",
       "      <td>LinkedIn · Yea Kang Yoon</td>\n",
       "      <td>yea-kang-yoon-3419415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70345</th>\n",
       "      <td>Artid Larson - Graduated with a bachelor's deg...</td>\n",
       "      <td>Graduated with a bachelor's degree in computer...</td>\n",
       "      <td>https://th.linkedin.com/in/artid-larson-a284151ba</td>\n",
       "      <td>LinkedIn ไทย</td>\n",
       "      <td>artid-larson-a284151ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70346</th>\n",
       "      <td>Majd Barhoum – Robotics Engineer, R&amp;D, ML</td>\n",
       "      <td>Robotics Engineer, R&amp;D, ML · An aspiring Robot...</td>\n",
       "      <td>https://ru.linkedin.com/in/majd-barhoum-b678b1115</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>majd-barhoum-b678b1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70347</th>\n",
       "      <td>‏Ifsan Ahammed‏ - ‏Student at Ajman University‏</td>\n",
       "      <td>Student at Ajman University‏ · التعليم: ‏Ajman...</td>\n",
       "      <td>https://ae.linkedin.com/in/ifsan-ahammed-92723...</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>ifsan-ahammed-927233385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70348</th>\n",
       "      <td>PanuWich Buttawong - Sripatum University</td>\n",
       "      <td>ประสบการณ์ · Internet Sales and Service Assist...</td>\n",
       "      <td>https://th.linkedin.com/in/panuwich-buttawong-...</td>\n",
       "      <td>LinkedIn ไทย</td>\n",
       "      <td>panuwich-buttawong-3575a1327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63516 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      Soohwan Lee 님 - Ph.D Candidate at UNIST | Huma...   \n",
       "1      Yongjae Lee 님 - Associate Professor at UNIST |...   \n",
       "2      이건모 님 - AI 엔지니어 인턴 @ Allganize , 울산과학기술원(UNIST...   \n",
       "3      Jaeho Kim 님 - Ph.D in Time Series | Korea Univ...   \n",
       "4      Robotics & AI Researcher |M.S. UNIST - Hyoungh...   \n",
       "...                                                  ...   \n",
       "70344  Yea Kang Yoon - Authentication AI/ML Product O...   \n",
       "70345  Artid Larson - Graduated with a bachelor's deg...   \n",
       "70346          Majd Barhoum – Robotics Engineer, R&D, ML   \n",
       "70347    ‏Ifsan Ahammed‏ - ‏Student at Ajman University‏   \n",
       "70348           PanuWich Buttawong - Sripatum University   \n",
       "\n",
       "                                                 snippet  \\\n",
       "0      I am a Ph.D. candidate in Design at UNIST, spe...   \n",
       "1      Associate Professor at UNIST | AI for Finance ...   \n",
       "2      AI 엔지니어 인턴 @ Allganize , 울산과학기술원(UNIST) 산업공학과 ...   \n",
       "3      Jaeho Kim is a Ph.D. Candidate at the Ulsan Na...   \n",
       "4      UNIST ASL에서 M.S. Student를 진행 중이며, 자율주행 연구실CAIL...   \n",
       "...                                                  ...   \n",
       "70344  Statistics. 2003 - 2006. Korea University Grap...   \n",
       "70345  Graduated with a bachelor's degree in computer...   \n",
       "70346  Robotics Engineer, R&D, ML · An aspiring Robot...   \n",
       "70347  Student at Ajman University‏ · التعليم: ‏Ajman...   \n",
       "70348  ประสบการณ์ · Internet Sales and Service Assist...   \n",
       "\n",
       "                                                    link  \\\n",
       "0               https://kr.linkedin.com/in/soohwanlee/ko   \n",
       "1      https://kr.linkedin.com/in/yongjae-lee-5489821...   \n",
       "2               https://kr.linkedin.com/in/keonmo-lee/ko   \n",
       "3                   https://kr.linkedin.com/in/jaeho3690   \n",
       "4                        https://kr.linkedin.com/in/hmue   \n",
       "...                                                  ...   \n",
       "70344  https://www.linkedin.com/in/yea-kang-yoon-3419415   \n",
       "70345  https://th.linkedin.com/in/artid-larson-a284151ba   \n",
       "70346  https://ru.linkedin.com/in/majd-barhoum-b678b1115   \n",
       "70347  https://ae.linkedin.com/in/ifsan-ahammed-92723...   \n",
       "70348  https://th.linkedin.com/in/panuwich-buttawong-...   \n",
       "\n",
       "                         source                   linkedin_id  \n",
       "0                           NaN                    soohwanlee  \n",
       "1                           NaN         yongjae-lee-548982107  \n",
       "2                           NaN                    keonmo-lee  \n",
       "3                           NaN                     jaeho3690  \n",
       "4                           NaN                          hmue  \n",
       "...                         ...                           ...  \n",
       "70344  LinkedIn · Yea Kang Yoon         yea-kang-yoon-3419415  \n",
       "70345              LinkedIn ไทย        artid-larson-a284151ba  \n",
       "70346                  LinkedIn        majd-barhoum-b678b1115  \n",
       "70347                  LinkedIn       ifsan-ahammed-927233385  \n",
       "70348              LinkedIn ไทย  panuwich-buttawong-3575a1327  \n",
       "\n",
       "[63516 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f8365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd534d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb793cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525471\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "df2 = pd.read_csv('../files/output_onlydog.csv')\n",
    "df3 = pd.read_csv('../files/output_backup.csv')\n",
    "\n",
    "merged = pd.concat([df2, df3], ignore_index=True)\n",
    "# clean = merged.drop_duplicates(subset=[\"author_id\"], keep='first')\n",
    "# print(len(clean))\n",
    "merged.to_csv(\"scholar_from_scholar_search_total.csv\", index=False)\n",
    "print(len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1099146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54540\n",
      "13803\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "prefix = \"scholar_search_results\"\n",
    "pattern = f\"{prefix}*.csv\"\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "df4 = pd.read_csv('scholar_total.csv')\n",
    "dfs = [df4]\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(e, path)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(len(merged))\n",
    "clean = merged.drop_duplicates(subset=[\"author_id\"], keep='first')\n",
    "print(len(clean))\n",
    "\n",
    "clean.to_csv(\"scholar_from_google.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1602033",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scholar_from_scholar_search_total.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 148\u001b[0m\n\u001b[1;32m    110\u001b[0m surnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGwak\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKwon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamgung\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDang\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHong\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHwa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHwang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHwangbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeung\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m ]\n\u001b[1;32m    131\u001b[0m surnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKwon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBae\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaek\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSon\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m ]\n\u001b[0;32m--> 148\u001b[0m from_scholar \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscholar_from_scholar_search_total.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_scholar : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(from_scholar))\n\u001b[1;32m    151\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(deep_learning_keywords)\n",
      "File \u001b[0;32m~/Desktop/sonus/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/sonus/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/sonus/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/sonus/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/sonus/myenv/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scholar_from_scholar_search_total.csv'"
     ]
    }
   ],
   "source": [
    "# 딥러닝 토픽이 논문 제목에 포함된 것만 남기기\n",
    "deep_learning_keywords = [\n",
    "    'llm',\n",
    "    'agent',\n",
    "    'large model',\n",
    "    'multimodal',\n",
    "    'adversarial network',\n",
    "    'generation',\n",
    "    \n",
    "    # Core DL fields\n",
    "    \"deep learning\",\n",
    "    'machine learning',\n",
    "    \"neural network\",\n",
    "    \"transformer\",\n",
    "    \"attention\",\n",
    "    \"autoregressive\",\n",
    "    \"self-supervised\",\n",
    "    \"contrastive learning\",\n",
    "    \"representation learning\",\n",
    "    \"data-driven\",\n",
    "    \"latent space\",\n",
    "    \"resnet\",\n",
    "    \"convnext\",\n",
    "    \"encoder\",\n",
    "    \"decoder\",\n",
    "    \"autoencoder\",\n",
    "    \"codec\",\n",
    "    \"nerf\",\n",
    "    \"gaussian\",\n",
    "    \"splatting\",\n",
    "    \"neural\",\n",
    "    \"super resolution\",\n",
    "\n",
    "    # Architectures\n",
    "    \"cnn\",\n",
    "    \"lstm\",\n",
    "    \"unet\",\n",
    "    \"vit\",\n",
    "    \"bert\",\n",
    "    \"gpt\",\n",
    "    \"vae\",\n",
    "    \"gan\",\n",
    "    \"vq-vae\",\n",
    "\n",
    "    # Generative models\n",
    "    \"diffusion\",\n",
    "    \"diffusion model\",\n",
    "    \"score-based\",\n",
    "    \"flow matching\",\n",
    "    \"normalizing flow\",\n",
    "    \"generative model\",\n",
    "    \n",
    "    # Multimodal\n",
    "    \"multimodal\",\n",
    "    \"vision-language\",\n",
    "    \"audio-language\",\n",
    "    \"clip model\",\n",
    "    \"large language model\",\n",
    "\n",
    "    # Audio / speech\n",
    "    \"tts\",\n",
    "    \"speech recognition\",\n",
    "    \"audio generation\",\n",
    "    \"music generation\",\n",
    "\n",
    "    \"backpropagation\",\n",
    "    \"gradient descent\",\n",
    "    \"finetuning\",\n",
    "    \"fine-tuning\",\n",
    "    \"pretraining\",\n",
    "    \"pre-training\",\n",
    "    \"zero-shot\",\n",
    "    \"zeroshot\",\n",
    "    \"few-shot\",\n",
    "    \"fewshot\",\n",
    "    \"kl divergence\",\n",
    "    \"cross entropy\",\n",
    "\n",
    "    \"reinforcement learning\",\n",
    "    \"policy gradient\",\n",
    "    \"ppo\",\n",
    "    \"grpo\",\n",
    "    \"rlhf\",\n",
    "    \"human feedback\",\n",
    "    \"reward model\",\n",
    "    \"graph\",\n",
    "    \"feedforward\",\n",
    "    \"feed-forward\",\n",
    "    \"backward\",\n",
    "    \"back-ward\",\n",
    "    \"representation\",\n",
    "    \"training \",\n",
    "    \"hyperparameter\",\n",
    "    \"hyper-parameter\",\n",
    "    \"supervised learning\",\n",
    "\n",
    "    \"retrieval\",\n",
    "    \"quantization\",\n",
    "    \"vlm \",\n",
    "    \"sllm\",\n",
    "\n",
    "    \"computer\",\n",
    "    \"algorithm\",\n",
    "\n",
    "    \"system\",\n",
    "    \"efficient\",\n",
    "    \"efficient\",\n",
    "]\n",
    "\n",
    "surnames = [\n",
    "    \"Kang\", \"Ko\", \"Gwak\", \"Gu\", \"Guk\", \"Kwon\", \"Keum\", \"Ki\",\n",
    "    \"Na\", \"Nam\", \"Namgung\", \"Noh\", \"Non\", \"Dan\", \"Dam\", \"Dang\",\n",
    "    \"Do\", \"Dokgo\", \"Dongbang\", \"Dong\", \"Du\", \"Ra\", \"Ryeong\",\n",
    "    \"Ryu\", \"Ryuk\", \"Ri\",\n",
    "    \"Ma\", \"Man\", \"Maeng\", \"Myeong\", \"Mo\", \"Mok\", \"Muk\", \"Moon\",\n",
    "    \"Min\", \"Park\", \"Ban\", \"Bang\", \"Bae\", \"Baek\", \"Beom\", \"Byun\",\n",
    "    \"Bok\", \"Bong\", \"Boo\", \"Bi\", \"Bin\",\n",
    "    \"Sa\", \"Sam\", \"Sang\", \"Seo\", \"Seomun\", \"Seon\", \"Seonwoo\", \"Sung\",\n",
    "    \"So\", \"Son\", \"Song\", \"Su\", \"Seung\", \"Si\", \"Shin\", \"Sim\",\n",
    "    \"A\", \"Ahn\", \"Ae\", \"Yang\", \"Eo\", \"Eom\", \"Yeo\", \"Yeon\", \"Yeom\",\n",
    "    \"Young\", \"Ye\", \"Oh\", \"Ok\", \"On\", \"Ong\", \"Wang\", \"Yo\", \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Wi\", \"Yu\", \"Yuk\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jeol\", \"Jung\", \"Je\", \"Jegal\",\n",
    "    \"Jo\", \"Jwa\", \"Joo\", \"Juk\", \"Jun\", \"Ji\", \"Jin\",\n",
    "    \"Cha\", \"Chae\", \"Cheo\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Chu\", \"Tak\", \"Tan\", \"Tang\", \"Tae\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\", \"Hyun\", \"Hyeong\", \"Ho\",\n",
    "    \"Hong\", \"Hwa\", \"Hwang\", \"Hwangbo\", \"Hu\", \"Heung\"\n",
    "]\n",
    "surnames = [\n",
    "    \"Kang\", \"Kwon\", \"Ko\"\n",
    "    \"Park\", \"Bang\", \"Bae\", \"Baek\",\n",
    "    \"Yong\",\n",
    "    \"Woo\", \"Won\", \"Yoon\", \"Eun\", \"Eum\",\n",
    "    \"Lee\", \n",
    "    \"In\", \"Lim\", \"Jang\", \"Jeon\", \"Jung\",\n",
    "    \"Jo\", \"Jin\",\n",
    "    \"Cha\", \"Cheon\", \"Cho\", \"Choi\",\n",
    "    \"Ha\", \"Hak\", \"Han\", \"Ham\", \"Heo\",\n",
    "    \"Kim\",\n",
    "    \"Hong\", \"Hwang\",\n",
    "    \"Shin\", \"Song\", \"Seo\",\n",
    "    \"Kwak\", \"Park\", \"Ryu\", \"Soh\", \"Roh\", \"Im\", \"Ahn\", \"Koh\",\n",
    "    \"Nam\", \"Oh\", \"Huh\", \"Son\"\n",
    "]\n",
    "\n",
    "from_scholar = pd.read_csv('scholar_from_scholar_search_total.csv')\n",
    "print(\"from_scholar : \", len(from_scholar))\n",
    "\n",
    "pattern = \"|\".join(deep_learning_keywords)\n",
    "\n",
    "from_scholard = from_scholar.drop_duplicates(subset=[\"author_id\"], keep='first')\n",
    "print(\"from_scholar_deep_learning : \", len(from_scholard))\n",
    "\n",
    "from_scholar_deep_learning = from_scholar[from_scholar[\"title\"].str.lower().str.contains(pattern, regex=True)]\n",
    "from_scholar_deep_learning = from_scholar_deep_learning.drop_duplicates(subset=[\"author_id\"], keep='first')\n",
    "print(\"from_scholar_deep_learning : \", len(from_scholar_deep_learning))\n",
    "\n",
    "pattern = \"|\".join([' ' + s.lower() for s in surnames])\n",
    "print(pattern)\n",
    "from_scholar_deep_learning_korean = from_scholar_deep_learning[from_scholar_deep_learning[\"author_names\"].str.lower().str.contains(pattern, regex=True)]\n",
    "\n",
    "print(len(from_scholar_deep_learning_korean), len(from_scholar_deep_learning), len(from_scholar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4fde60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17883/17883 [00:00<00:00, 48188.03it/s]\n",
      "100%|██████████| 13803/13803 [00:00<00:00, 73175.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31686\n",
      "27790\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "datas = []\n",
    "\n",
    "from_google = pd.read_csv('scholar_from_google.csv')\n",
    "\n",
    "for i in tqdm(range(len(from_scholar_deep_learning_korean))):\n",
    "    d = from_scholar_deep_learning_korean.iloc[i]\n",
    "    datas.append({\n",
    "        'name': d['author_names'],\n",
    "        'author_id': d['author_id'],\n",
    "    })\n",
    "\n",
    "for i in tqdm(range(len(from_google))):\n",
    "    d = from_google.iloc[i]\n",
    "    datas.append({\n",
    "        'name': d['name'],\n",
    "        'author_id': d['author_id'],\n",
    "    })\n",
    "print(len(datas))\n",
    "\n",
    "newdf = pd.DataFrame(datas)\n",
    "newdf = newdf.drop_duplicates(subset=['author_id'], keep='first')\n",
    "print(len(newdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c0d61179",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv(\"scholar_from_google_deep_learning_korean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34a450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced8447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
