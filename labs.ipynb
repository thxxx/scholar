{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e74834-d7a8-43c8-8283-7378f0dc1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException, JavascriptException\n",
    "from selenium_stealth import stealth\n",
    "from bs4 import BeautifulSoup\n",
    "from utils import build_driver\n",
    "\n",
    "drv = build_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcbb3f-0e7d-4017-b1fb-bffce84e1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_explain_link(drv, pathto):\n",
    "    drv.get(pathto)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 1) 팝업 닫기 (Selenium WebElement로 직접)\n",
    "    try:\n",
    "        close_btn = WebDriverWait(drv, 3).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \".pop-ad .btn-close\"))\n",
    "        )\n",
    "        close_btn.click()\n",
    "        time.sleep(1)  # 닫힌 후 UI 안정화\n",
    "        print(\"Exist\")\n",
    "    except (TimeoutException, NoSuchElementException) as e:\n",
    "        print(\"No\", e)\n",
    "        pass  # 팝업이 없으면 그냥 넘어감\n",
    "\n",
    "    # 2) 팝업 닫은 후 HTML 다시 읽기\n",
    "    html = drv.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        # 3) 정상 파싱\n",
    "        professor_profile = soup.find('div', class_='professor-profile')\n",
    "        lab_link = professor_profile.find('p', class_='site').find(\"a\")['href']\n",
    "    \n",
    "        main = soup.find('ul', class_='open-lab-data-list')\n",
    "        content = main.find_all('li')[0].find('div', class_='open-lab-content-area')\n",
    "    \n",
    "        return lab_link, content\n",
    "    except Exception as e:\n",
    "        print(\"Error \", e)\n",
    "        return \"\", \"\"\n",
    "\n",
    "datas = []\n",
    "# 68까지\n",
    "\n",
    "for i in tqdm(range(1, 200)):\n",
    "    url = f'https://phdkim.net/professor/open-lab?page={i}'\n",
    "\n",
    "    try:\n",
    "        drv.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        html = drv.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        lab_list = soup.find('div', class_='open-lab-list')\n",
    "        if not lab_list:\n",
    "            print(\"END! At \", i)\n",
    "            break\n",
    "        ais = lab_list.find_all('a', class_='clearfix')\n",
    "        print(\"Len ais \", len(ais))\n",
    "        for idx in range(len(ais)):\n",
    "            dpath = 'https://phdkim.net' + ais[idx]['href']\n",
    "            lablink, content = get_explain_link(drv, dpath)\n",
    "\n",
    "            getd = {\n",
    "                'detail_path': dpath,\n",
    "                'university': ais[idx].find(\"p\", class_='university').text.strip(),\n",
    "                'lablink': lablink,\n",
    "                'content': content\n",
    "            }\n",
    "            datas.append(getd)\n",
    "        \n",
    "        df = pd.DataFrame(datas)\n",
    "        df.to_csv(\"phdlabs2.csv\", index=False)\n",
    "        \n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at {i}th \", e)\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb55712-6797-4e6d-aae9-f3564a93fdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88101a-1531-4b97-be99-2d280d1cc1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a4533-26f8-48c3-b2b1-81b351da6934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff5fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crawlbase\n",
      "  Downloading crawlbase-1.0.0.tar.gz (6.6 kB)\n",
      "Using legacy 'setup.py install' for crawlbase, since package 'wheel' is not installed.\n",
      "Installing collected packages: crawlbase\n",
      "    Running setup.py install for crawlbase ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed crawlbase-1.0.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/gimhojin/Desktop/sonus/myenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install crawlbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawlbase import CrawlingAPI\n",
    "import json\n",
    "\n",
    "# Initialize Crawlbase API with your access token\n",
    "crawling_api = CrawlingAPI({ 'token': '' })\n",
    "\n",
    "URL = 'https://www.linkedin.com/in/hojin-kim-2a553222a/'\n",
    "\n",
    "options = {\n",
    "    'scraper': 'linkedin-profile',\n",
    "    'async': 'true'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0b5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = crawling_api.get(URL, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83835d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headers': {'original_status': 'None', 'pc_status': '200', 'url': 'None'},\n",
       " 'status_code': 200,\n",
       " 'body': b'Please visit the below link to enable LinkedIn crawling. \\nhttps://crawlbase.com/dashboard/account/domain/linkedin/agreement \\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response['status_code'] == 200:\n",
    "    return json.loads(response['body'].decode('utf-8'))\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response['status_code'])\n",
    "    return None\n",
    "\n",
    "def scrape_profile(url):\n",
    "try:\n",
    "    json_response = make_crawlbase_request(url)\n",
    "    print(\"json_response : \", json_response)\n",
    "    if json_response:\n",
    "        return json_response\n",
    "except Exception as e:\n",
    "    print(f\"Request failed: {e}\")\n",
    "\n",
    "return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "scraped_data = scrape_profile(URL)\n",
    "print(json.dumps(scraped_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6341eeef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjson_response\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json_response' is not defined"
     ]
    }
   ],
   "source": [
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465df8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638b2542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (1.2.3)\n",
      "Requirement already satisfied: filelock in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/gimhojin/Desktop/sonus/myenv/lib/python3.9/site-packages (from typer-slim->huggingface_hub) (8.1.8)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98fb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 19.7MB / 19.7MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n"
     ]
    }
   ],
   "source": [
    "huggingface_api_key = \"\"\n",
    "\n",
    "def upload_to_huggingface(file_path):\n",
    "    from huggingface_hub import HfApi\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    api = HfApi()\n",
    "    login(token=huggingface_api_key)\n",
    "\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file_path,\n",
    "        path_in_repo=file_path.split(\"/\")[-1],\n",
    "        repo_id=\"Daniel777/harper\",\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "\n",
    "upload_to_huggingface(\"./google_serp/linkedins_total_1216.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed702f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
